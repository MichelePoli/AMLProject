{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDNiYPiZj2eE",
        "outputId": "00ad6969-0f44-4e9e-e12b-34c7645c45cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->thop) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.2)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from kornia) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.1->kornia) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
            "Downloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia_rs, kornia\n",
            "Successfully installed kornia-0.8.0 kornia_rs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install thop\n",
        "!pip install yacs\n",
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Id9h3F1jzIh"
      },
      "outputs": [],
      "source": [
        "#import\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from thop import profile, clever_format\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "import logging\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AQAM9O9ieOq",
        "outputId": "850fa8d1-9fc7-4432-fd2b-d51f9739f8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-20 14:19:20--  https://zenodo.org/record/5706578/files/Train.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Train.zip [following]\n",
            "--2025-01-20 14:19:21--  https://zenodo.org/records/5706578/files/Train.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘Train.zip’\n",
            "\n",
            "Train.zip           100%[===================>]   3.75G  17.9MB/s    in 3m 38s  \n",
            "\n",
            "2025-01-20 14:22:59 (17.6 MB/s) - ‘Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "--2025-01-20 14:22:59--  https://zenodo.org/record/5706578/files/Val.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Val.zip [following]\n",
            "--2025-01-20 14:23:00--  https://zenodo.org/records/5706578/files/Val.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘Val.zip’\n",
            "\n",
            "Val.zip             100%[===================>]   2.26G  11.8MB/s    in 3m 24s  \n",
            "\n",
            "2025-01-20 14:26:24 (11.4 MB/s) - ‘Val.zip’ saved [2425958254/2425958254]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/5706578/files/Train.zip\n",
        "!wget https://zenodo.org/record/5706578/files/Val.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjxZm9J70AVT",
        "outputId": "907ddd72-da90-4543-9257-367bb6b676e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-20 14:26:24--  https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38061375 (36M) [application/octet-stream]\n",
            "Saving to: ‘PIDNet_S_ImageNet.pth.tar’\n",
            "\n",
            "PIDNet_S_ImageNet.p 100%[===================>]  36.30M  10.8MB/s    in 4.2s    \n",
            "\n",
            "2025-01-20 14:26:29 (8.70 MB/s) - ‘PIDNet_S_ImageNet.pth.tar’ saved [38061375/38061375]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZP8ABHifPf",
        "outputId": "d2cf907a-1815-4680-f880-bda15321242e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Directory Contents: ['Train']\n",
            "Val Directory Contents: ['Val']\n"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "train_zip = \"Train.zip\"\n",
        "val_zip = \"Val.zip\"\n",
        "\n",
        "# Extract Train.zip\n",
        "with zipfile.ZipFile(train_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Train\")\n",
        "\n",
        "# Extract Val.zip\n",
        "with zipfile.ZipFile(val_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Val\")\n",
        "\n",
        "\n",
        "# Verify the extracted folders\n",
        "print(\"Train Directory Contents:\", os.listdir(\"Train\"))\n",
        "print(\"Val Directory Contents:\", os.listdir(\"Val\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p_CFAn8Fig5e"
      },
      "outputs": [],
      "source": [
        "#TASK 5\n",
        "\n",
        "COLOR_MAP = {\n",
        "    'Background': 0.00392157,\n",
        "    'Building': 0.00784314,\n",
        "    'Road': 0.01176471,\n",
        "    'Water': 0.01568628,\n",
        "    'Barren': 0.01960784,\n",
        "    'Forest': 0.02352941,\n",
        "    'Agricultural': 0.02745098\n",
        "}\n",
        "\n",
        "LABEL_MAP = OrderedDict(\n",
        "    Background=0,\n",
        "    Building=1,\n",
        "    Road=2,\n",
        "    Water=3,\n",
        "    Barren=4,\n",
        "    Forest=5,\n",
        "    Agricultural=6\n",
        ")\n",
        "\n",
        "# Tolerance-based color to label conversion\n",
        "def color_to_one_hot(mask, label_map, tolerance=0.001, num_classes=7):\n",
        "    mask = np.asarray(mask, dtype=np.float32)\n",
        "    if len(mask.shape) == 2:  # Ensure channel dimension exists\n",
        "        mask = np.expand_dims(mask, axis=0)  # Convert [H, W] to [1, H, W]\n",
        "\n",
        "    one_hot_mask = np.zeros((num_classes, mask.shape[1], mask.shape[2]), dtype=np.float32)\n",
        "\n",
        "    for class_name, class_index in label_map.items():\n",
        "        color_value = COLOR_MAP[class_name]\n",
        "        # Match pixels with the grayscale value within the tolerance\n",
        "        matches = np.abs(mask - color_value) < tolerance\n",
        "        one_hot_mask[class_index, np.squeeze(matches, axis=0)] = 1.0\n",
        "\n",
        "    return torch.from_numpy(one_hot_mask)\n",
        "\n",
        "\n",
        "# Define a function to denormalize the image\n",
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\" Denormalize the tensor back to the [0, 1] range for visualization. \"\"\"\n",
        "    for i in range(len(mean)):\n",
        "        tensor[i] = tensor[i] * std[i] + mean[i]\n",
        "    return tensor\n",
        "\n",
        "def show_image(image_tensor):\n",
        "    \"\"\" Display a transformed image using matplotlib. \"\"\"\n",
        "    image_tensor = image_tensor.cpu()  # Sposta il tensore sulla CPU\n",
        "    image_tensor = denormalize(image_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Denormalize the image\n",
        "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert to HxWxC format\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "def show_mask(mask_tensor, num_classes=7):\n",
        "    \"\"\" Display the mask using matplotlib. \"\"\"\n",
        "    # Convert the one-hot mask to the label mask (indices of the highest value per pixel)\n",
        "    mask = mask_tensor.squeeze().cpu().numpy()  # Remove the batch dimension and convert to numpy\n",
        "    label_mask = np.argmax(mask, axis=0)  # Get the class index with the highest value for each pixel\n",
        "\n",
        "    plt.imshow(label_mask, cmap='tab20')  # Use a colormap suitable for categorical data\n",
        "    plt.colorbar()  # Optionally add a color bar\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def colorJitter(colorJitter, img_mean, data = None, target = None, s=0.25):\n",
        "    # s is the strength of colorjitter\n",
        "    #colorJitter\n",
        "    if not (data is None):\n",
        "        if data.shape[1]==3:\n",
        "            if colorJitter > 0.2:\n",
        "                img_mean, _ = torch.broadcast_tensors(img_mean.unsqueeze(0).unsqueeze(2).unsqueeze(3), data)\n",
        "                seq = nn.Sequential(kornia.augmentation.ColorJitter(brightness=s,contrast=s,saturation=s,hue=s))\n",
        "                data = (data+img_mean)/255\n",
        "                data = seq(data)\n",
        "                data = (data*255-img_mean).float()\n",
        "    return data, target\n",
        "\n",
        "def gaussian_blur(blur, data = None, target = None):\n",
        "    if not (data is None):\n",
        "        if data.shape[1]==3:\n",
        "            if blur > 0.5:\n",
        "                sigma = np.random.uniform(0.15,1.15)\n",
        "                kernel_size_y = int(np.floor(np.ceil(0.1 * data.shape[2]) - 0.5 + np.ceil(0.1 * data.shape[2]) % 2))\n",
        "                kernel_size_x = int(np.floor(np.ceil(0.1 * data.shape[3]) - 0.5 + np.ceil(0.1 * data.shape[3]) % 2))\n",
        "                kernel_size = (kernel_size_y, kernel_size_x)\n",
        "                seq = nn.Sequential(kornia.filters.GaussianBlur2d(kernel_size=kernel_size, sigma=(sigma, sigma)))\n",
        "                data = seq(data)\n",
        "    return data, target\n",
        "\n",
        "def strongTransform(parameters, data=None, target=None):\n",
        "    assert ((data is not None) or (target is not None))\n",
        "    # data, target = transformsgpu.oneMix(mask = parameters[\"Mix\"], data = data, target = target)\n",
        "    data, target = colorJitter(colorJitter = parameters[\"ColorJitter\"], img_mean = torch.from_numpy(IMG_MEAN.copy()).cuda(), data = data, target = target)\n",
        "    data, target = gaussian_blur(blur = parameters[\"GaussianBlur\"], data = data, target = target)\n",
        "    # data, target = transformsgpu.flip(flip = parameters[\"flip\"], data = data, target = target)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform_image=None, transform_label=None, subdir=\"Rural\", aug1=False, aug2=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_label = transform_label\n",
        "        self.aug1 = aug1\n",
        "        self.aug2 = aug2\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        image_dir = os.path.join(root_dir, split, subdir, 'images_png')\n",
        "        label_dir = os.path.join(root_dir, split, subdir, 'masks_png')\n",
        "\n",
        "        for f in os.listdir(image_dir):\n",
        "            if f.endswith('.png'):\n",
        "                self.image_paths.append(os.path.join(image_dir, f))\n",
        "                self.label_paths.append(os.path.join(label_dir, f))\n",
        "\n",
        "    def __len__(self):\n",
        "        original_length = len(self.image_paths)\n",
        "        if self.aug1 and self.aug2:\n",
        "            return original_length * 2\n",
        "        elif self.aug1 or self.aug2:\n",
        "            return int(original_length * 1.5)\n",
        "        else:\n",
        "            return original_length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_length = len(self.image_paths)\n",
        "\n",
        "        # Calcola l'indice originale nel dataset\n",
        "        idx = idx % original_length\n",
        "\n",
        "        aug_1 = False\n",
        "        aug_2 = False\n",
        "        if(idx < original_length):\n",
        "            aug_1 = False\n",
        "            aug_2 = False\n",
        "\n",
        "        elif(idx >= original_length):\n",
        "            if(random.random() < 0.5):\n",
        "                if(self.aug1):\n",
        "                    aug_1 = True\n",
        "                    aug_2 = False\n",
        "                elif(self.aug2):\n",
        "                    aug_1 = False\n",
        "                    aug_2 = True\n",
        "\n",
        "            else:\n",
        "                if (self.aug2):\n",
        "                    aug_1 = False\n",
        "                    aug_2 = True\n",
        "                elif(self.aug1):\n",
        "                    aug_1 = True\n",
        "                    aug_2 = False\n",
        "\n",
        "\n",
        "\n",
        "        # Carica l'immagine e la maschera\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = Image.open(self.label_paths[idx]).convert(\"L\")\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        label_np = np.array(label)\n",
        "\n",
        "        # Seed per garantire coerenza tra immagine e maschera\n",
        "        seed = np.random.randint(2147483647)\n",
        "\n",
        "        # Applica augmentazione se abilitata\n",
        "        if self.aug1 and aug_1:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            transform_aug1 = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(30)\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_aug1(image)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_aug1(label)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        if self.aug2 and aug_2:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "\n",
        "            strong_parameters = {}\n",
        "            strong_parameters[\"ColorJitter\"] = random.uniform(0, 1)\n",
        "            strong_parameters[\"GaussianBlur\"] = random.uniform(0, 1)\n",
        "\n",
        "\n",
        "            image = Image.fromarray(image_np)\n",
        "            image, _ = strongTransform(strong_parameters, image, None)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        # Trasformazioni opzionali per immagine e maschera\n",
        "        if self.transform_image:\n",
        "            transform_image_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_image_alb(image)\n",
        "\n",
        "        if self.transform_label:\n",
        "            transform_label_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_label_alb(label).squeeze(0)\n",
        "            label = color_to_one_hot(label, LABEL_MAP)\n",
        "\n",
        "        return image,label\n",
        "\n",
        "# Define transformations without augmentations\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Set augmentation flags\n",
        "AUG1 = True\n",
        "AUG2 = True\n",
        "\n",
        "BATCH_SIZE = 16 #or 32, or 64 ...\n",
        "NUM_WORKERS = 2 # Number of cpu cores\n",
        "PIN_MEMORY = True\n",
        "PERSISTENT_WORKERS = False # Set true if you have persistent workers issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DUTFu81y7PO",
        "outputId": "b1c14615-80d9-43bb-e5f8-d07cde6167d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 386 (delta 131), reused 125 (delta 125), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 212.80 MiB | 16.89 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n",
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XuJiacong/PIDNet.git\n",
        "%cd content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g-Lia_DZS6_k"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/PIDNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PQ5Ioe3yt3P",
        "outputId": "af06d1c0-e1eb-42e2-a075-f98b9bc405e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-43ad49fc9a51>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n"
          ]
        }
      ],
      "source": [
        "from models.model_utils import BasicBlock, Bottleneck, segmenthead, DAPPM, PAPPM, PagFM, Bag, Light_Bag\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_\n",
        "\n",
        "\n",
        "def get_seg_model(name, num_classes, imgnet_pretrained, model_pth):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "\n",
        "        pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    else:\n",
        "\n",
        "        pretrained_dict = torch.load(model_pth, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_seg_model(name='pidnet_s', num_classes=7, imgnet_pretrained=True, model_pth='PIDNet_S_ImageNet.pth.tar').to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vloYGWlwZmE",
        "outputId": "88bb358d-5fc1-4a41-c6e0-6601b267e71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------- LOSS TYPE:    cross_entropy    -----------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-43ad49fc9a51>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n",
            "Epoch 1/20: 100%|██████████| 62/62 [02:44<00:00,  2.66s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Loss: 2.2734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 1/20: 100%|██████████| 62/62 [01:22<00:00,  1.33s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Validation mIoU: 0.1157\n",
            "Epoch [1/20] - Per-Class IoU: {'Background': 0.17642688751220703, 'Building': 0.10554836690425873, 'Road': 0.12387628853321075, 'Water': 0.2400778830051422, 'Barren': 0.029657654464244843, 'Forest': 0.08047422021627426, 'Agricultural': 0.053529608994722366}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 62/62 [02:47<00:00,  2.70s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20] - Loss: 1.2009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 2/20: 100%|██████████| 62/62 [01:23<00:00,  1.35s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20] - Validation mIoU: 0.1514\n",
            "Epoch [2/20] - Per-Class IoU: {'Background': 0.2570488154888153, 'Building': 0.2849400043487549, 'Road': 0.1688736081123352, 'Water': 0.22335393726825714, 'Barren': 0.02376559190452099, 'Forest': 0.10174041986465454, 'Agricultural': 0.00018410604388918728}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 62/62 [02:51<00:00,  2.77s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20] - Loss: 1.2403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 3/20: 100%|██████████| 62/62 [01:26<00:00,  1.39s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20] - Validation mIoU: 0.1999\n",
            "Epoch [3/20] - Per-Class IoU: {'Background': 0.2080235481262207, 'Building': 0.32607072591781616, 'Road': 0.2459224909543991, 'Water': 0.47277453541755676, 'Barren': 0.0499999076128006, 'Forest': 0.09225597977638245, 'Agricultural': 0.004373033996671438}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 62/62 [03:04<00:00,  2.98s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20] - Loss: 1.2116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 4/20: 100%|██████████| 62/62 [01:23<00:00,  1.35s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20] - Validation mIoU: 0.1947\n",
            "Epoch [4/20] - Per-Class IoU: {'Background': 0.11215827614068985, 'Building': 0.3884633481502533, 'Road': 0.18913942575454712, 'Water': 0.47441214323043823, 'Barren': 0.1204519271850586, 'Forest': 0.07802267372608185, 'Agricultural': 0.0005044182180427015}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20: 100%|██████████| 62/62 [02:53<00:00,  2.80s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20] - Loss: 1.1764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 5/20: 100%|██████████| 62/62 [01:25<00:00,  1.37s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20] - Validation mIoU: 0.1969\n",
            "Epoch [5/20] - Per-Class IoU: {'Background': 0.10189169645309448, 'Building': 0.33819010853767395, 'Road': 0.24402974545955658, 'Water': 0.5057864189147949, 'Barren': 0.07979550212621689, 'Forest': 0.07627340406179428, 'Agricultural': 0.03231905773282051}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20: 100%|██████████| 62/62 [02:53<00:00,  2.81s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20] - Loss: 1.2217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 6/20: 100%|██████████| 62/62 [01:24<00:00,  1.37s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20] - Validation mIoU: 0.1846\n",
            "Epoch [6/20] - Per-Class IoU: {'Background': 0.09548769146203995, 'Building': 0.19282802939414978, 'Road': 0.18723681569099426, 'Water': 0.3965190351009369, 'Barren': 0.04045450687408447, 'Forest': 0.07060206681489944, 'Agricultural': 0.3087942600250244}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20: 100%|██████████| 62/62 [03:01<00:00,  2.92s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20] - Loss: 1.1290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 7/20: 100%|██████████| 62/62 [01:20<00:00,  1.30s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20] - Validation mIoU: 0.2072\n",
            "Epoch [7/20] - Per-Class IoU: {'Background': 0.08751541376113892, 'Building': 0.24797576665878296, 'Road': 0.1960144191980362, 'Water': 0.46374422311782837, 'Barren': 0.047390371561050415, 'Forest': 0.07006856799125671, 'Agricultural': 0.33796945214271545}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 62/62 [02:53<00:00,  2.80s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20] - Loss: 1.0217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 8/20: 100%|██████████| 62/62 [01:22<00:00,  1.32s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20] - Validation mIoU: 0.2000\n",
            "Epoch [8/20] - Per-Class IoU: {'Background': 0.09212468564510345, 'Building': 0.20482785999774933, 'Road': 0.1859332025051117, 'Water': 0.5050326585769653, 'Barren': 0.11905838549137115, 'Forest': 0.0717003121972084, 'Agricultural': 0.22154326736927032}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20: 100%|██████████| 62/62 [02:56<00:00,  2.84s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20] - Loss: 1.0812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 9/20: 100%|██████████| 62/62 [01:30<00:00,  1.46s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20] - Validation mIoU: 0.1735\n",
            "Epoch [9/20] - Per-Class IoU: {'Background': 0.09297969937324524, 'Building': 0.20173320174217224, 'Road': 0.11158378422260284, 'Water': 0.4582146406173706, 'Barren': 0.11914662271738052, 'Forest': 0.070440873503685, 'Agricultural': 0.1600920706987381}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20: 100%|██████████| 62/62 [03:02<00:00,  2.94s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20] - Loss: 0.8763\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 10/20: 100%|██████████| 62/62 [01:23<00:00,  1.34s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20] - Validation mIoU: 0.2284\n",
            "Epoch [10/20] - Per-Class IoU: {'Background': 0.09904129803180695, 'Building': 0.24681200087070465, 'Road': 0.22812117636203766, 'Water': 0.5152268409729004, 'Barren': 0.017797643318772316, 'Forest': 0.07107316702604294, 'Agricultural': 0.4204387664794922}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 62/62 [02:47<00:00,  2.71s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20] - Loss: 0.8250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 11/20: 100%|██████████| 62/62 [01:29<00:00,  1.45s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20] - Validation mIoU: 0.2193\n",
            "Epoch [11/20] - Per-Class IoU: {'Background': 0.10823561996221542, 'Building': 0.20825053751468658, 'Road': 0.22826945781707764, 'Water': 0.5100362300872803, 'Barren': 0.010666626505553722, 'Forest': 0.06994093954563141, 'Agricultural': 0.3996464014053345}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20: 100%|██████████| 62/62 [02:51<00:00,  2.76s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20] - Loss: 0.8962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 12/20: 100%|██████████| 62/62 [01:25<00:00,  1.38s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20] - Validation mIoU: 0.2459\n",
            "Epoch [12/20] - Per-Class IoU: {'Background': 0.09875169396400452, 'Building': 0.2350584864616394, 'Road': 0.23883534967899323, 'Water': 0.547697901725769, 'Barren': 0.07360102236270905, 'Forest': 0.07449058443307877, 'Agricultural': 0.45283350348472595}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 62/62 [02:59<00:00,  2.89s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20] - Loss: 0.9301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 13/20: 100%|██████████| 62/62 [01:22<00:00,  1.33s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20] - Validation mIoU: 0.2420\n",
            "Epoch [13/20] - Per-Class IoU: {'Background': 0.10011056810617447, 'Building': 0.2574138045310974, 'Road': 0.23269225656986237, 'Water': 0.5382202863693237, 'Barren': 0.07150765508413315, 'Forest': 0.07202553004026413, 'Agricultural': 0.4217430353164673}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20: 100%|██████████| 62/62 [02:50<00:00,  2.75s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/20] - Loss: 1.0170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 14/20: 100%|██████████| 62/62 [01:25<00:00,  1.37s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/20] - Validation mIoU: 0.2349\n",
            "Epoch [14/20] - Per-Class IoU: {'Background': 0.10094710439443588, 'Building': 0.22991672158241272, 'Road': 0.2431904524564743, 'Water': 0.5069629549980164, 'Barren': 0.07582619041204453, 'Forest': 0.0734119862318039, 'Agricultural': 0.41373443603515625}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/20: 100%|██████████| 62/62 [02:52<00:00,  2.79s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20] - Loss: 0.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 15/20: 100%|██████████| 62/62 [01:25<00:00,  1.38s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20] - Validation mIoU: 0.2322\n",
            "Epoch [15/20] - Per-Class IoU: {'Background': 0.09548764675855637, 'Building': 0.21728035807609558, 'Road': 0.2256760597229004, 'Water': 0.4808898866176605, 'Barren': 0.08390870690345764, 'Forest': 0.07274096459150314, 'Agricultural': 0.4492385685443878}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20: 100%|██████████| 62/62 [02:55<00:00,  2.82s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20] - Loss: 0.9145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 16/20: 100%|██████████| 62/62 [01:27<00:00,  1.42s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20] - Validation mIoU: 0.2385\n",
            "Epoch [16/20] - Per-Class IoU: {'Background': 0.0837794616818428, 'Building': 0.2692708373069763, 'Road': 0.2340131402015686, 'Water': 0.5013859272003174, 'Barren': 0.04401249438524246, 'Forest': 0.07219201326370239, 'Agricultural': 0.46457135677337646}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20: 100%|██████████| 62/62 [02:52<00:00,  2.77s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20] - Loss: 0.8506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 17/20: 100%|██████████| 62/62 [01:22<00:00,  1.32s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20] - Validation mIoU: 0.2257\n",
            "Epoch [17/20] - Per-Class IoU: {'Background': 0.10186842083930969, 'Building': 0.2087026983499527, 'Road': 0.226219043135643, 'Water': 0.4964049160480499, 'Barren': 0.0611659437417984, 'Forest': 0.0708712562918663, 'Agricultural': 0.4144364595413208}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/20: 100%|██████████| 62/62 [02:52<00:00,  2.78s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20] - Loss: 0.6957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 18/20: 100%|██████████| 62/62 [01:22<00:00,  1.33s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20] - Validation mIoU: 0.2342\n",
            "Epoch [18/20] - Per-Class IoU: {'Background': 0.09238442778587341, 'Building': 0.2527294456958771, 'Road': 0.2413594126701355, 'Water': 0.442514568567276, 'Barren': 0.08548452705144882, 'Forest': 0.07484821230173111, 'Agricultural': 0.4501507580280304}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19/20: 100%|██████████| 62/62 [02:54<00:00,  2.81s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20] - Loss: 0.7671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 19/20: 100%|██████████| 62/62 [01:27<00:00,  1.41s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20] - Validation mIoU: 0.1852\n",
            "Epoch [19/20] - Per-Class IoU: {'Background': 0.09420181810855865, 'Building': 0.25256508588790894, 'Road': 0.2327226996421814, 'Water': 0.14705246686935425, 'Barren': 0.07145697623491287, 'Forest': 0.03676437586545944, 'Agricultural': 0.4614200294017792}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20/20: 100%|██████████| 62/62 [02:52<00:00,  2.79s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20] - Loss: 0.7624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating Epoch 20/20: 100%|██████████| 62/62 [01:31<00:00,  1.48s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20] - Validation mIoU: 0.1809\n",
            "Epoch [20/20] - Per-Class IoU: {'Background': 0.09358514845371246, 'Building': 0.2530990242958069, 'Road': 0.2292770892381668, 'Water': 0.15091192722320557, 'Barren': 0.029479313641786575, 'Forest': 0.04602381959557533, 'Agricultural': 0.4639444649219513}\n",
            "Best Validation mIoU: 0.2459\n",
            "_______________________________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from utils.criterion import OhemCrossEntropy\n",
        "import gc\n",
        "\n",
        "\n",
        "num_classes=7\n",
        "# Define mIoU metric\n",
        "jaccard = JaccardIndex(task=\"multiclass\", num_classes=7).to('cuda')  # intersection over union. Directly measures the overlap between predicted segmentation and ground truth.\n",
        "\n",
        "# DACS implementation (adapting from the pseudocode)\n",
        "class DACSModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DACSModel, self).__init__()\n",
        "        self.model = get_seg_model(name='pidnet_s', num_classes=7, imgnet_pretrained=True, model_pth='PIDNet_S_ImageNet.pth.tar').to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def dice_loss(predictions, labels, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Computes the Dice Loss between the predicted and ground truth labels.\n",
        "    Dice loss is used to measure the similarity between two sets, used for segmentation tasks.\n",
        "    \"\"\"\n",
        "    # Convert predictions and labels to binary masks\n",
        "    pred_binary = torch.argmax(predictions, dim=1)  # Shape: (batch_size, height, width)\n",
        "    label_binary = labels  # Shape: (batch_size, height, width)\n",
        "\n",
        "    # Flatten the predicted and ground truth masks\n",
        "    intersection = torch.sum(pred_binary * label_binary)  # Intersection (True positives)\n",
        "    total = torch.sum(pred_binary) + torch.sum(label_binary)  # Total (sum of both sets)\n",
        "\n",
        "    # Dice coefficient\n",
        "    dice = (2. * intersection + smooth) / (total + smooth)\n",
        "\n",
        "    # Dice loss is the complement of the Dice coefficient (1 - Dice)\n",
        "    return 1 - dice\n",
        "\n",
        "\n",
        "def boundary_loss(predictions, labels, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Computes the boundary loss between the predicted and ground truth boundaries.\n",
        "    Boundary loss is typically computed using the boundary distance between the predicted mask and ground truth mask.\n",
        "    \"\"\"\n",
        "    # Create binary masks from predictions and labels\n",
        "    pred_binary = torch.argmax(predictions, dim=1)  # Shape: (batch_size, height, width)\n",
        "    label_binary = labels  # Shape: (batch_size, height, width)\n",
        "\n",
        "    # Get boundary of the predicted and ground truth masks using distance transform\n",
        "    pred_boundary = F.conv2d(pred_binary.unsqueeze(1).float(), weight=torch.ones(1, 1, 3, 3).to(device), stride=1, padding=1)\n",
        "    label_boundary = F.conv2d(label_binary.unsqueeze(1).float(), weight=torch.ones(1, 1, 3, 3).to(device), stride=1, padding=1)\n",
        "\n",
        "    # Compute the boundary loss (L1 loss on boundary maps)\n",
        "    boundary_loss = F.mse_loss(pred_boundary, label_boundary)\n",
        "    \n",
        "    return boundary_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_loss(pred_source, label_source, pred_mixed, label_mixed, lambda_param, loss_type=\"cross_entropy\",  perturbation=None):\n",
        "    # criterion = OhemCrossEntropy()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Cross-entropy loss for source domain and mixed domain\n",
        "    if loss_type == \"cross_entropy\":\n",
        "        loss_source = criterion(pred_source, label_source)\n",
        "        loss_mixed = criterion(pred_mixed, label_mixed)\n",
        "    # Boundary loss\n",
        "    elif loss_type == \"boundary\":\n",
        "        loss_source = boundary_loss(pred_source, label_source)\n",
        "        loss_mixed = boundary_loss(pred_mixed, label_mixed)\n",
        "    \n",
        "    # Dice loss\n",
        "    elif loss_type == \"dice\":\n",
        "        loss_source = dice_loss(pred_source, label_source)\n",
        "        loss_mixed = dice_loss(pred_mixed, label_mixed)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown loss type\")\n",
        "\n",
        "    return loss_source + lambda_param * loss_mixed\n",
        "\n",
        "\n",
        "def adjust_mask(binary_mask, source_labels, target_proportion=0.2):\n",
        "    \"\"\"\n",
        "    Regola la maschera in modo da ridurre la quantità di pixel source\n",
        "    quando la proporzione è troppo alta, e bilancia la proporzione tra source e target.\n",
        "\n",
        "    Args:\n",
        "    - binary_mask (Tensor): la maschera binaria che indica quali pixel provengono dalla source image.\n",
        "    - source_labels (Tensor): le etichette della source image.\n",
        "    - target_proportion (float): la proporzione di target che vogliamo ottenere dopo l'aggiornamento.\n",
        "\n",
        "    Returns:\n",
        "    - binary_mask (Tensor): la maschera aggiornata.\n",
        "    \"\"\"\n",
        "    # Calcoliamo la proporzione attuale della source image\n",
        "    proportion_source = binary_mask.float().mean().item()\n",
        "\n",
        "    print(f\"Proporzione source prima dell'aggiustamento: {proportion_source:.2f}\")\n",
        "\n",
        "    if proportion_source == 1.0:\n",
        "        # Se tutti i pixel sono source, vogliamo ridurre la quantità di pixel source\n",
        "        # Se la proporzione source è 1, significa che tutti i pixel sono della source image.\n",
        "        # Abbiamo bisogno di \"trasformare\" alcuni di questi pixel in target per bilanciare la proporzione.\n",
        "\n",
        "        # Determina quanti pixel devono essere cambiati per raggiungere la proporzione target desiderata\n",
        "        target_pixels_needed = int(binary_mask.numel() * target_proportion)\n",
        "\n",
        "        # Troviamo gli indici dei pixel source (True) nella maschera\n",
        "        source_pixel_indices = binary_mask.nonzero(as_tuple=True)[0]\n",
        "\n",
        "        # Calcoliamo quanti pixel dobbiamo convertire in target (False)\n",
        "        num_to_convert = source_pixel_indices.size(0) - target_pixels_needed\n",
        "\n",
        "        if num_to_convert > 0:\n",
        "            # Scegliamo casualmente i pixel da modificare da source a target\n",
        "            indices_to_convert = source_pixel_indices[:num_to_convert]\n",
        "\n",
        "            # Impostiamo questi pixel su False (target)\n",
        "            binary_mask[indices_to_convert] = False\n",
        "        else:\n",
        "            print(\"Non è necessario modificare la maschera, la proporzione è già adatta.\")\n",
        "\n",
        "    # Dopo l'aggiornamento, la proporzione di source dovrebbe essere bilanciata\n",
        "    proportion_source = binary_mask.float().mean().item()\n",
        "    print(f\"Proporzione source dopo l'aggiustamento: {proportion_source:.2f}\")\n",
        "\n",
        "    return binary_mask\n",
        "\n",
        "def classmix(source_images, source_labels, target_images, target_labels, num_classes=7):\n",
        "    \"\"\"\n",
        "    Perform classmix image augmentation, mixing source domain image and target domain image pixels based on class masks.\n",
        "    \"\"\"\n",
        "    batch_size, _, height, width = source_images.size()\n",
        "    #print(\"sourceLabels\",source_labels)\n",
        "\n",
        "    # Ensure source_labels is an integer type\n",
        "    source_labels = source_labels.long()\n",
        "\n",
        "    # Generate a random binary mask for each class\n",
        "    class_masks = torch.zeros((batch_size, height, width), device=source_images.device, dtype=torch.bool)\n",
        "    #print(class_masks)\n",
        "    #print(class_masks.shape) #torch.Size([2, 512, 512])\n",
        "    #print(source_labels.shape) #torch.Size([2, 7, 512, 512])\n",
        "    #source_labels = torch.argmax(source_labels, dim=1)  # Convert to class indices (batch_size, height, width)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        best_proportion_source = 0.0  # To store the best proportion found\n",
        "        best_class_mask = torch.zeros((height, width), device=source_images.device, dtype=torch.bool)  # Best mask found\n",
        "        count = 0  # Initialize the count for while loop iterations\n",
        "        proportion_source=0.0\n",
        "\n",
        "\n",
        "        # Keep regenerating the mask until the proportion is within the desired range or count reaches 10\n",
        "        while (proportion_source < 0.1 or proportion_source > 0.9) and count < 10:\n",
        "            # Randomly select half of the classes for each image in the batch\n",
        "            selected_classes = torch.randperm(num_classes)[:num_classes // 2]\n",
        "            #print(\"selected_classes: \", selected_classes)\n",
        "\n",
        "            # Reset the mask for this batch\n",
        "            class_masks[b] = torch.zeros((height, width), device=source_images.device, dtype=torch.bool)\n",
        "\n",
        "            for cls in selected_classes:\n",
        "                class_masks[b] = class_masks[b] | (source_labels[b] == cls)\n",
        "\n",
        "            # Calculate the proportion of source pixels in the mask\n",
        "            proportion_source = class_masks[b].float().mean()\n",
        "            #print(\"proportion_source:\", proportion_source)\n",
        "\n",
        "            # If this proportion is the best so far, store the mask and proportion\n",
        "            if proportion_source > best_proportion_source:\n",
        "                best_proportion_source = proportion_source\n",
        "                best_class_mask = class_masks[b].clone()  # Store the best mask\n",
        "\n",
        "            # Increment the count\n",
        "            count += 1\n",
        "\n",
        "            # If the count reaches 10, exit the loop\n",
        "            if count >= 10:\n",
        "                #print(f\"Maximum iterations reached for batch {b}. Proportion did not satisfy the condition.\")\n",
        "                break\n",
        "\n",
        "        # After the loop, use the best mask found\n",
        "        #print(f\"Best proportion_source for batch {b}: {best_proportion_source}\")\n",
        "        class_mask_rgb = best_class_mask.unsqueeze(0).repeat(3, 1, 1).float() * 255  # Convert to RGB\n",
        "        #show_image(class_mask_rgb) #print the mask\n",
        "\n",
        "        # Update the class mask for this batch\n",
        "        class_masks[b] = best_class_mask\n",
        "\n",
        "    # Expand the mask to match the channel dimension for images\n",
        "    class_masks_expanded = class_masks.unsqueeze(1).expand(-1, source_images.size(1), -1, -1)\n",
        "\n",
        "    # Mix images using the masks\n",
        "    mixed_images = torch.where(class_masks_expanded, source_images, target_images)\n",
        "\n",
        "\n",
        "\n",
        "    # Mix labels using the same masks\n",
        "    mixed_labels = torch.where(class_masks, source_labels, target_labels)\n",
        "\n",
        "\n",
        "    return mixed_images, mixed_labels\n",
        "\n",
        "def calculate_miou(predictions, labels, num_classes=7):\n",
        "    \"\"\"\n",
        "    Compute Mean Intersection over Union (mIoU) for a given batch of mixed images.\n",
        "    Args:\n",
        "    - predictions (torch.Tensor): Predicted labels for each pixel in the batch (B, H, W)\n",
        "    - labels (torch.Tensor): Ground truth labels for each pixel in the batch (B, H, W)\n",
        "    - num_classes (int): The total number of classes for segmentation\n",
        "    Returns:\n",
        "    - mean_iou (float): The mean Intersection over Union (mIoU) for the batch\n",
        "    \"\"\"\n",
        "    iou_per_class = []\n",
        "\n",
        "    # Loop over each class\n",
        "    for cls in range(num_classes):\n",
        "        # Predicted pixels belonging to the class\n",
        "        pred_class = (predictions == cls)\n",
        "        # Ground truth pixels belonging to the class\n",
        "        true_class = (labels == cls)\n",
        "\n",
        "        # Intersection (True Positive)\n",
        "        intersection = torch.sum(pred_class & true_class).float()\n",
        "        # Union (True Positive + False Positive + False Negative)\n",
        "        union = torch.sum(pred_class | true_class).float()\n",
        "\n",
        "        # IoU for the class\n",
        "        if union != 0:\n",
        "            iou = intersection / union\n",
        "        else:\n",
        "            iou = torch.tensor(0.0)\n",
        "\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    # Calculate the mean IoU for the batch\n",
        "    mean_iou = torch.mean(torch.stack(iou_per_class))\n",
        "    return mean_iou\n",
        "\n",
        "def calculate_lambda(predictions, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Calculate the adaptive lambda based on the proportion of confident pixels.\n",
        "\n",
        "    Args:\n",
        "    - predictions (torch.Tensor): Model predictions (logits or probabilities) [batch_size, num_classes, H, W].\n",
        "    - threshold (float): Confidence threshold for determining confident pixels.\n",
        "\n",
        "    Returns:\n",
        "    - lambda_value (float): Adaptive lambda for the current batch.\n",
        "    \"\"\"\n",
        "    # Convert logits to probabilities (if necessary)\n",
        "    #print(\"predictions: \", predictions.shape) #torch.Size([2, 7, 512, 512])\n",
        "    probs = F.softmax(predictions, dim=1)  # [batch_size, num_classes, H, W] #we apply the softmax\n",
        "    #print(\"probs: \", probs.shape) #torch.Size([2, 7, 512, 512])\n",
        "\n",
        "    # Get maximum probabilities for each pixel\n",
        "    max_probs, _ = probs.max(dim=1)  # [batch_size, H, W]\n",
        "\n",
        "    # Create a mask of confident pixels\n",
        "    confident_mask = max_probs > threshold  # [batch_size, H, W]\n",
        "\n",
        "    # Calculate the proportion of confident pixels for each image in the batch\n",
        "    confident_proportions = confident_mask.float().mean(dim=(1, 2))  # [batch_size]\n",
        "\n",
        "    # Average over the batch to get the lambda value\n",
        "    lambda_value = confident_proportions.mean().item()\n",
        "\n",
        "    return lambda_value\n",
        "\n",
        "# DACS training loop\n",
        "def train_dacs_and_validate(model, source_loader, target_loader,loss_type, num_epochs=20, lambda_param=0.7, lr=1e-4, confidence_threshold=0.7  ):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)  # Move model to device (CUDA or CPU)\n",
        "    model.train()\n",
        "    best_miou = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Iterate through both source and target datasets for training\n",
        "        train_progress_bar = tqdm(zip(source_loader, target_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\", total=min(len(source_loader), len(target_loader)))\n",
        "        for batch_idx, (source_data, target_data) in enumerate(train_progress_bar):\n",
        "\n",
        "            source_images, source_labels = source_data\n",
        "            #print(\"source_images: \", source_images.shape)\n",
        "            if source_images.shape[0]==1: # when we run subset\n",
        "              continue\n",
        "            target_images, _ = target_data  # Target labels are not available, will use pseudo-labels\n",
        "\n",
        "            # Move to device\n",
        "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
        "            target_images = target_images.to(device)\n",
        "\n",
        "            # 1. Forward pass through the model for the source and target domains\n",
        "            source_preds = model(source_images)\n",
        "            main_output_source= source_preds[1]\n",
        "\n",
        "            #print(\"main_output_source shape : \", main_output_source.shape)\n",
        "            main_output_source = F.interpolate(main_output_source, size=(512,512), mode='bilinear', align_corners=True)\n",
        "            #print(\"main_output_source shape : \", main_output_source.shape) #torch.Size([2, 7, 512, 512])\n",
        "            #print(\"logits\", main_output_source[0, :, 0, 0]) #example: tensor([-3.4636, -3.5338,  2.0996, -0.4825, -0.7998,  1.7863,  2.0860], device='cuda:0', grad_fn=<SelectBackward0>)\n",
        "\n",
        "\n",
        "            # 2. Generate pseudo-label for target domain\n",
        "            target_preds = model(target_images)\n",
        "            main_output_target = target_preds[1]\n",
        "            #print(\"main_output_target shape : \", main_output_target.shape)\n",
        "            main_output_target = F.interpolate(main_output_target, size=(512,512), mode='bilinear', align_corners=True)\n",
        "            #print(\"main_output_target shape : \", main_output_target.shape) #torch.Size([2, 7, 512, 512])\n",
        "\n",
        "            #The goal of pseudo-labeling is to assign a single class label to each pixel based on the highest probability (or logit). The argmax function finds the index of the maximum value along the specified dimension (num_classes). The result tensor is (batch_size, height, width)\n",
        "            pseudo_labels = torch.argmax(main_output_target, dim=1).to(device)\n",
        "            #print(\"pseudo_labels\", pseudo_labels.shape) #torch.Size([2, 512, 512])\n",
        "            #print(\"source labels :\", source_labels) #here each class label is represented as a one-hot vector for each pixel across the image. we want convert the one-hot encoded source_labels into class indices, so each pixel gets assigned the class marked as 1 in the one-hot vector.\n",
        "\n",
        "            #If a pixel in source_labels has a one-hot encoded value like [1, 0, 0, 0], this means that the pixel belongs to class 0 (the first class). ->\n",
        "            source_labels = torch.argmax(source_labels, dim=1).to(device)\n",
        "            #print(\"source_labels\", source_labels.shape) # torch.Size([2, 512, 512])\n",
        "\n",
        "            #3. Calculate adaptive lambda\n",
        "            #lambda_param = calculate_lambda(main_output_target, threshold=confidence_threshold)\n",
        "            #train_progress_bar.set_postfix({'lambda': lambda_param})\n",
        "\n",
        "            # 4. MIX source and target images and labels (DACS augmentation step)\n",
        "            mixed_images, mixed_labels = classmix(source_images, source_labels, target_images, pseudo_labels)\n",
        "            #print(\"mixed_labels \", mixed_labels.shape)\n",
        "            mixed_labels=mixed_labels.to(device)\n",
        "            #print(\"mixed_labels\", mixed_labels.shape)\n",
        "\n",
        "            # Visualization: Save and show one example from the batch\n",
        "            # if (batch_idx == 61):  # Show only for the last batch of each epoch. We can try only for 1 batch in 1 epoch at max, otherwise output is truncated.\n",
        "#                 print(f\"Visualizing images from batch {batch_idx} at epoch {epoch+1}:\")\n",
        "#                 for i in range(16):\n",
        "#                     print(f\"Source Image {i + 1}:\")\n",
        "#                     show_image(source_images[i])\n",
        "#                     print(f\"Target Image {i + 1}:\")\n",
        "#                     show_image(target_images[i])\n",
        "#                     print(f\"Mixed Image {i + 1}:\")\n",
        "#                     show_image(mixed_images[i])\n",
        "\n",
        "\n",
        "            # Step 6: Forward pass\n",
        "            mixed_preds = model(mixed_images)\n",
        "            main_output_mixed = mixed_preds[1]\n",
        "            main_output_mixed = F.interpolate(main_output_mixed, size=(512, 512), mode='bilinear', align_corners=True)\n",
        "\n",
        "            # Calculate the loss\n",
        "            # loss = dacs_loss(main_output_source, source_labels, main_output_mixed, mixed_labels, lambda_param)\n",
        "            # Esempio di utilizzo\n",
        "\n",
        "            # pred_source, label_source, pred_mixed, label_mixed, lambda_param\n",
        "            loss = compute_loss(main_output_source, source_labels, main_output_mixed, mixed_labels, lambda_param=0.5,\n",
        "                            loss_type=loss_type)\n",
        "\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print loss and mIoU after each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_miou = 0.0\n",
        "        val_class_iou = torch.zeros(len(LABEL_MAP)).to('cuda')  # Store IoU per class\n",
        "        val_class_counts = torch.zeros(len(LABEL_MAP)).to('cuda')  # To track number of pixels for each class\n",
        "\n",
        "        val_progress_bar = tqdm(target_loader, desc=f\"Validating Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\", total=len(target_loader))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_progress_bar:\n",
        "                images, masks = images.to('cuda'), masks.to('cuda')\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)  # [batch_size, num_classes, 512, 512] #outputs obtained with model trained on mixed images.\n",
        "                main_output = outputs[1]\n",
        "                main_output = F.interpolate(main_output, size=(512,512), mode='bilinear', align_corners=True)\n",
        "                preds = torch.argmax(main_output, dim=1)  # [batch_size, 512, 512] #select the class with max probability for each pixel\n",
        "\n",
        "                # Convert masks to class indices format if one-hot encoded\n",
        "                if masks.ndim == 4:  # [batch_size, num_classes, height, width]\n",
        "                    masks = masks.argmax(dim=1)  # [batch_size, height, width]\n",
        "\n",
        "                # Calculate per-class IoU\n",
        "                for c, class_name in LABEL_MAP.items():  # Iterate over LABEL_MAP classes\n",
        "                    true_class = (masks == class_name)\n",
        "                    pred_class = (preds == class_name)\n",
        "\n",
        "                    intersection = torch.sum(true_class & pred_class).float()\n",
        "                    union = torch.sum(true_class | pred_class).float()\n",
        "\n",
        "                    if union != 0:\n",
        "                        val_class_iou[class_name] += intersection / union\n",
        "                    val_class_counts[class_name] += 1\n",
        "\n",
        "                # Calculate overall mIoU for this batch\n",
        "                val_miou += jaccard(preds, masks)\n",
        "\n",
        "        # Average metrics\n",
        "        val_miou /= len(target_loader)\n",
        "\n",
        "        # Calculate average IoU for each class\n",
        "        avg_class_iou = val_class_iou / val_class_counts\n",
        "\n",
        "        # Print validation metrics at the end of each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation mIoU: {val_miou:.4f}\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Per-Class IoU: {dict(zip(LABEL_MAP.keys(), avg_class_iou.tolist()))}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_miou > best_miou:\n",
        "            best_miou = val_miou\n",
        "            torch.save(model.state_dict(), 'best_pidnet_model.pth')\n",
        "\n",
        "    print(f\"Best Validation mIoU: {best_miou:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loading datasets\n",
        "source_dataset = LoveDADataset(root_dir='Train/', split='Train', transform_image=transform_image, transform_label=transform_label, subdir='Urban', aug1=AUG1, aug2=AUG2)\n",
        "target_dataset = LoveDADataset(root_dir='Val/', split='Val', transform_image=transform_image, transform_label=transform_label, subdir='Rural')\n",
        "\n",
        "source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=PERSISTENT_WORKERS)\n",
        "target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "\n",
        "# Initialize, train the model and validate\n",
        "model = DACSModel(num_classes=7).to(device)  # Assuming 7 classes\n",
        "\n",
        "\n",
        "# loss_types = [\"cross_entropy\"]\n",
        "loss_types = [\"boundary\"]\n",
        "# loss_types = [\"dice\" ]\n",
        "\n",
        "for l in loss_types:\n",
        "    print(\"------------------------------------- LOSS TYPE:   \", l, \"   -----------------------------------\")\n",
        "\n",
        "    train_dacs_and_validate(model, source_loader, target_loader, l, num_epochs=20)\n",
        "\n",
        "    print(\"_______________________________________________________________________________________________________________________\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9oya4zJFpDt",
        "outputId": "e0e875eb-1ed3-4399-aa5f-4faa240f8992"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.reset_max_memory_allocated() #in caso di 'cuda out of memory'\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "import gc\n",
        "gc.collect() #clear cpu memory\n",
        "import torch\n",
        "torch.cuda.empty_cache() #clear gpu memory #runna più volte\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vs-hJOV9TGK1"
      },
      "outputs": [],
      "source": [
        "# unsqueeze function in PyTorch is used to add an extra dimension to a tensor.\n",
        "# channels: 3 color channels (RGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJLWYZ3YWgrO"
      },
      "source": [
        "DACS method:\n",
        "1. ClassMix strategy:\n",
        "Half of the classes from a source domain image are selected, and the corresponding pixels are cut out and pasted onto an image from the target domain.\n",
        "The same mixing procedure is applied to the labels (pseudo-labels for the target domain and ground-truth labels for the source domain).\n",
        "2. Pseudo-labels:\n",
        "The target domain image is passed through the model before mixing to produce pseudo-labels. These labels are then mixed similarly to how the images are mixed.\n",
        "3. Cross-domain Mixing:\n",
        "By combining the two domains at the pixel level, this method attempts to make it harder for the model to learn to discern between the domains. This can mitigate the issue of class distribution differences between source and target domains.\n",
        "\n",
        "Key Steps for Implementation:\n",
        "1. Image Mixing:\n",
        "Select a portion of the source domain image and mix it with the target domain image.\n",
        "2. Label Mixing:\n",
        "Use the ground-truth label from the source domain and pseudo-labels from the target domain and mix them similarly to the images.\n",
        "3. Model Training:\n",
        "Once the mixed images and labels are prepared, the model can be trained using the usual supervised learning approach, where predictions are made and loss is computed using cross-entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFR854e-56Xe"
      },
      "source": [
        "1. Training Phase:\n",
        "\n",
        "Input: Source images and source labels.\n",
        "\n",
        "Output: Predicted masks and loss between predictions and source labels.\n",
        "\n",
        "Augment with target data (via classmix).\n",
        "\n",
        "Backpropagation and optimization.\n",
        "\n",
        "2. Validation Phase:\n",
        "\n",
        "Input: Target images (no labels for target during training).\n",
        "\n",
        "Output: Predicted masks for the target images.\n",
        "\n",
        "Compute metrics such as mIoU between predicted masks and true target domain masks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
