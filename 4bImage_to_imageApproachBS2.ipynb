{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDNiYPiZj2eE",
        "outputId": "0a6c142c-1547-4b9f-8d00-16dc028125eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from thop import profile, clever_format\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "import logging\n",
        "\n"
      ],
      "metadata": {
        "id": "9Id9h3F1jzIh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/5706578/files/Train.zip\n",
        "!wget https://zenodo.org/record/5706578/files/Val.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AQAM9O9ieOq",
        "outputId": "ece2fc7f-a221-4b12-e546-75b14385ccc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 21:37:40--  https://zenodo.org/record/5706578/files/Train.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Train.zip [following]\n",
            "--2025-01-09 21:37:40--  https://zenodo.org/records/5706578/files/Train.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘Train.zip’\n",
            "\n",
            "Train.zip           100%[===================>]   3.75G  13.0MB/s    in 4m 58s  \n",
            "\n",
            "2025-01-09 21:42:39 (12.9 MB/s) - ‘Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "--2025-01-09 21:42:39--  https://zenodo.org/record/5706578/files/Val.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.43.25, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Val.zip [following]\n",
            "--2025-01-09 21:42:39--  https://zenodo.org/records/5706578/files/Val.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘Val.zip’\n",
            "\n",
            "Val.zip             100%[===================>]   2.26G  20.2MB/s    in 2m 2s   \n",
            "\n",
            "2025-01-09 21:44:41 (19.0 MB/s) - ‘Val.zip’ saved [2425958254/2425958254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjxZm9J70AVT",
        "outputId": "ec6a46de-50ac-4cf1-f460-56296ba697c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 21:44:41--  https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38061375 (36M) [application/octet-stream]\n",
            "Saving to: ‘PIDNet_S_ImageNet.pth.tar’\n",
            "\n",
            "PIDNet_S_ImageNet.p 100%[===================>]  36.30M  12.5MB/s    in 2.9s    \n",
            "\n",
            "2025-01-09 21:44:45 (12.5 MB/s) - ‘PIDNet_S_ImageNet.pth.tar’ saved [38061375/38061375]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths\n",
        "train_zip = \"Train.zip\"\n",
        "val_zip = \"Val.zip\"\n",
        "\n",
        "# Extract Train.zip\n",
        "with zipfile.ZipFile(train_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Train\")\n",
        "\n",
        "# Extract Val.zip\n",
        "with zipfile.ZipFile(val_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Val\")\n",
        "\n",
        "\n",
        "# Verify the extracted folders\n",
        "print(\"Train Directory Contents:\", os.listdir(\"Train\"))\n",
        "print(\"Val Directory Contents:\", os.listdir(\"Val\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZP8ABHifPf",
        "outputId": "abec5096-ee52-4366-df40-941c9443cd4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Directory Contents: ['Train']\n",
            "Val Directory Contents: ['Val']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 3.b\n",
        "\n",
        "COLOR_MAP = {\n",
        "    'Background': 0.00392157,\n",
        "    'Building': 0.00784314,\n",
        "    'Road': 0.01176471,\n",
        "    'Water': 0.01568628,\n",
        "    'Barren': 0.01960784,\n",
        "    'Forest': 0.02352941,\n",
        "    'Agricultural': 0.02745098\n",
        "}\n",
        "\n",
        "LABEL_MAP = OrderedDict(\n",
        "    Background=0,\n",
        "    Building=1,\n",
        "    Road=2,\n",
        "    Water=3,\n",
        "    Barren=4,\n",
        "    Forest=5,\n",
        "    Agricultural=6\n",
        ")\n",
        "\n",
        "# Tolerance-based color to label conversion\n",
        "def color_to_one_hot(mask, label_map, tolerance=0.001, num_classes=7):\n",
        "    mask = np.asarray(mask, dtype=np.float32)\n",
        "    if len(mask.shape) == 2:  # Ensure channel dimension exists\n",
        "        mask = np.expand_dims(mask, axis=0)  # Convert [H, W] to [1, H, W]\n",
        "\n",
        "    one_hot_mask = np.zeros((num_classes, mask.shape[1], mask.shape[2]), dtype=np.float32)\n",
        "\n",
        "    for class_name, class_index in label_map.items():\n",
        "        color_value = COLOR_MAP[class_name]\n",
        "        # Match pixels with the grayscale value within the tolerance\n",
        "        matches = np.abs(mask - color_value) < tolerance\n",
        "        one_hot_mask[class_index, np.squeeze(matches, axis=0)] = 1.0\n",
        "\n",
        "    return torch.from_numpy(one_hot_mask)\n",
        "\n",
        "\n",
        "# Define a function to denormalize the image\n",
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\" Denormalize the tensor back to the [0, 1] range for visualization. \"\"\"\n",
        "    for i in range(len(mean)):\n",
        "        tensor[i] = tensor[i] * std[i] + mean[i]\n",
        "    return tensor\n",
        "\n",
        "def show_image(image_tensor):\n",
        "    \"\"\" Display a transformed image using matplotlib. \"\"\"\n",
        "    image_tensor = denormalize(image_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Denormalize the image\n",
        "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert to HxWxC format\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "def show_mask(mask_tensor, num_classes=7):\n",
        "    \"\"\" Display the mask using matplotlib. \"\"\"\n",
        "    # Convert the one-hot mask to the label mask (indices of the highest value per pixel)\n",
        "    mask = mask_tensor.squeeze().cpu().numpy()  # Remove the batch dimension and convert to numpy\n",
        "    label_mask = np.argmax(mask, axis=0)  # Get the class index with the highest value for each pixel\n",
        "\n",
        "    plt.imshow(label_mask, cmap='tab20')  # Use a colormap suitable for categorical data\n",
        "    plt.colorbar()  # Optionally add a color bar\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform_image=None, transform_label=None, subdir=\"Rural\", aug1=False, aug2=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_label = transform_label\n",
        "        self.aug1 = aug1\n",
        "        self.aug2 = aug2\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        image_dir = os.path.join(root_dir, split, subdir, 'images_png')\n",
        "        label_dir = os.path.join(root_dir, split, subdir, 'masks_png')\n",
        "\n",
        "        for f in os.listdir(image_dir):\n",
        "            if f.endswith('.png'):\n",
        "                self.image_paths.append(os.path.join(image_dir, f))\n",
        "                self.label_paths.append(os.path.join(label_dir, f))\n",
        "\n",
        "    def __len__(self):\n",
        "        original_length = len(self.image_paths)\n",
        "        if self.aug1 and self.aug2:\n",
        "            return original_length * 2\n",
        "        elif self.aug1 or self.aug2:\n",
        "            return int(original_length * 1.5)\n",
        "        else:\n",
        "            return original_length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_length = len(self.image_paths)\n",
        "\n",
        "        # Calcola l'indice originale nel dataset\n",
        "        idx = idx % original_length\n",
        "\n",
        "        aug_1 = False\n",
        "        aug_2 = False\n",
        "        if(idx < original_length):\n",
        "            aug_1 = False\n",
        "            aug_2 = False\n",
        "\n",
        "        elif(idx >= original_length):\n",
        "            if(random.random() < 0.5):\n",
        "                if(self.aug1):\n",
        "                    aug_1 = True\n",
        "                elif(self.aug2):\n",
        "                    aug_2 = True\n",
        "            else:\n",
        "                if (self.aug2):\n",
        "                    aug_2 = True\n",
        "                elif(self.aug1):\n",
        "                    aug_1 = True\n",
        "\n",
        "\n",
        "        # Carica l'immagine e la maschera\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = Image.open(self.label_paths[idx]).convert(\"L\")\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        label_np = np.array(label)\n",
        "\n",
        "        # Seed per garantire coerenza tra immagine e maschera\n",
        "        seed = np.random.randint(2147483647)\n",
        "\n",
        "        # Applica augmentazione se abilitata\n",
        "        if self.aug1 and aug_1:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            transform_aug1 = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(30)\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_aug1(image)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_aug1(label)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        if self.aug2 and aug_2:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            transform_aug2 = transforms.Compose([\n",
        "                #sigma=(0.1, 5) indica un intervallo di valori tra 0.1 e 5, quindi la sfocatura potrebbe variare in base a come viene applicata.\n",
        "                transforms.GaussianBlur(kernel_size=(5,9), sigma=( 0.1, 5)) #valori di dimensioni (5, 9), che significa che il filtro si estende su un'area di 5x9 pixel.\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_aug2(image)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_aug2(label)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        # Trasformazioni opzionali per immagine e maschera\n",
        "        if self.transform_image:\n",
        "            transform_image_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_image_alb(image)\n",
        "\n",
        "        if self.transform_label:\n",
        "            transform_label_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_label_alb(label).squeeze(0)\n",
        "            label = color_to_one_hot(label, LABEL_MAP)\n",
        "\n",
        "        return image,label\n",
        "\n",
        "# Define transformations without augmentations\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Set augmentation flags\n",
        "AUG1 = True\n",
        "AUG2 = True\n",
        "\n",
        "BATCH_SIZE = 2 #or 32, or 64 ...\n",
        "NUM_WORKERS = 2 # Number of cpu cores\n",
        "PIN_MEMORY = True\n",
        "PERSISTENT_WORKERS = False # Set true if you have persistent workers issues\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p_CFAn8Fig5e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/XuJiacong/PIDNet.git\n",
        "%cd content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DUTFu81y7PO",
        "outputId": "dccbe1fc-d62d-4f05-fc60-58a0f0451b5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 386 (delta 131), reused 125 (delta 125), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 212.80 MiB | 24.95 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n",
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/PIDNet')"
      ],
      "metadata": {
        "id": "g-Lia_DZS6_k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.model_utils import BasicBlock, Bottleneck, segmenthead, DAPPM, PAPPM, PagFM, Bag, Light_Bag\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape[-1]) #512\n",
        "        #print(x.shape[-2])\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_\n",
        "\n",
        "\n",
        "def get_seg_model(name, num_classes, imgnet_pretrained, model_pth):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "\n",
        "        pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    else:\n",
        "        # print(\"vamos\")\n",
        "\n",
        "        pretrained_dict = torch.load(model_pth, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_seg_model(name='pidnet_s', num_classes=7, imgnet_pretrained=True, model_pth='PIDNet_S_ImageNet.pth.tar').to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PQ5Ioe3yt3P",
        "outputId": "b587b063-e58b-4579-dec7-8c89828b715c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-34d80a4c5901>:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#device=\"cpu\"\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmr4SlruwpAN",
        "outputId": "62733508-6715-4781-cd38-f60ee6e242f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "num_classes=7\n",
        "# Define mIoU metric\n",
        "jaccard = JaccardIndex(task=\"multiclass\", num_classes=7).to('cuda')  # intersection over union. Directly measures the overlap between predicted segmentation and ground truth.\n",
        "\n",
        "# DACS implementation (adapting from the pseudocode)\n",
        "class DACSModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DACSModel, self).__init__()\n",
        "        self.model = get_seg_model(name='pidnet_s', num_classes=7, imgnet_pretrained=True, model_pth='PIDNet_S_ImageNet.pth.tar').to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def dacs_loss(pred_source, label_source, pred_mixed, label_mixed, lambda_param): #from the paper.\n",
        "    # Cross-entropy loss for source domain and mixed domain\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #print(\"pred_source.shape: \", pred_source.shape)\n",
        "    #print(\"label_source.shape: \", label_source.shape)\n",
        "    #print(\"pred_mixed.shape: \", pred_mixed.shape)\n",
        "    #print(\"label_mixed.shape: \", label_mixed.shape)\n",
        "\n",
        "    # Calculate loss for both source and mixed data\n",
        "    loss_source = criterion(pred_source, label_source)\n",
        "    #print(\"loss_source: \", loss_source)\n",
        "    loss_mixed = criterion(pred_mixed, label_mixed)\n",
        "    #print(\"loss_mixed: \", loss_mixed)\n",
        "\n",
        "    return loss_source + lambda_param * loss_mixed\n",
        "\n",
        "\n",
        "def classmix(source_images, source_labels, target_images, target_labels, num_classes=7):\n",
        "    \"\"\"\n",
        "    Perform classmix image augmentation, mixing source domain image and target domain image pixels based on class masks.\n",
        "    \"\"\"\n",
        "    batch_size, _, height, width = source_images.size()\n",
        "    #print(\"sourceLabels\",source_labels)\n",
        "\n",
        "    # Ensure source_labels is an integer type\n",
        "    source_labels = source_labels.long()\n",
        "\n",
        "    # Generate a random binary mask for each class\n",
        "    class_masks = torch.zeros((batch_size, height, width), device=source_images.device, dtype=torch.bool)\n",
        "    #print(class_masks)\n",
        "    #print(class_masks.shape) #torch.Size([2, 512, 512])\n",
        "    #print(source_labels.shape) #torch.Size([2, 7, 512, 512])\n",
        "    #source_labels = torch.argmax(source_labels, dim=1)  # Convert to class indices (batch_size, height, width)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        # Randomly select half of the classes\n",
        "        selected_classes = torch.randperm(num_classes)[:num_classes // 2]\n",
        "        for cls in selected_classes:\n",
        "            class_masks[b] = class_masks[b] | (source_labels[b] == cls)\n",
        "\n",
        "    # Expand the mask to match the channel dimension for images\n",
        "    class_masks_expanded = class_masks.unsqueeze(1).expand(-1, source_images.size(1), -1, -1)\n",
        "\n",
        "    # Mix images using the masks\n",
        "    mixed_images = torch.where(class_masks_expanded, source_images, target_images)\n",
        "\n",
        "    # Mix labels using the same masks\n",
        "    mixed_labels = torch.where(class_masks, source_labels, target_labels)\n",
        "\n",
        "\n",
        "    return mixed_images, mixed_labels\n",
        "\n",
        "\n",
        "def calculate_miou(predictions, labels, num_classes=7):\n",
        "    \"\"\"\n",
        "    Compute Mean Intersection over Union (mIoU) for a given batch of mixed images.\n",
        "    Args:\n",
        "    - predictions (torch.Tensor): Predicted labels for each pixel in the batch (B, H, W)\n",
        "    - labels (torch.Tensor): Ground truth labels for each pixel in the batch (B, H, W)\n",
        "    - num_classes (int): The total number of classes for segmentation\n",
        "    Returns:\n",
        "    - mean_iou (float): The mean Intersection over Union (mIoU) for the batch\n",
        "    \"\"\"\n",
        "    iou_per_class = []\n",
        "\n",
        "    # Loop over each class\n",
        "    for cls in range(num_classes):\n",
        "        # Predicted pixels belonging to the class\n",
        "        pred_class = (predictions == cls)\n",
        "        # Ground truth pixels belonging to the class\n",
        "        true_class = (labels == cls)\n",
        "\n",
        "        # Intersection (True Positive)\n",
        "        intersection = torch.sum(pred_class & true_class).float()\n",
        "        # Union (True Positive + False Positive + False Negative)\n",
        "        union = torch.sum(pred_class | true_class).float()\n",
        "\n",
        "        # IoU for the class\n",
        "        if union != 0:\n",
        "            iou = intersection / union\n",
        "        else:\n",
        "            iou = torch.tensor(0.0)\n",
        "\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    # Calculate the mean IoU for the batch\n",
        "    mean_iou = torch.mean(torch.stack(iou_per_class))\n",
        "    return mean_iou\n",
        "\n",
        "def validate(model, target_loader, num_classes=7): #we give it the model trained on mixed images and mixed labels\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_miou = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation needed for validation\n",
        "        for target_data in target_loader:\n",
        "            target_images, target_labels = target_data\n",
        "            target_images, target_labels = target_images.to(device), target_labels.to(device)\n",
        "\n",
        "            # 1. Forward pass for target domain images\n",
        "            target_preds = model(target_images)\n",
        "            main_output_target = target_preds[1]\n",
        "            main_output_target = F.interpolate(main_output_target, size=(512, 512), mode='bilinear', align_corners=True)\n",
        "\n",
        "            # 2. Calculate mIoU for target domain\n",
        "            miou_batch = calculate_miou(main_output_target, target_labels, num_classes)\n",
        "            running_miou += miou_batch.item()\n",
        "            total_samples += 1\n",
        "\n",
        "    mean_miou = running_miou / total_samples\n",
        "    print(f\"Validation mIoU: {mean_miou:.4f}\")\n",
        "    return mean_miou\n",
        "\n",
        "def calculate_lambda(predictions, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Calculate the adaptive lambda based on the proportion of confident pixels.\n",
        "\n",
        "    Args:\n",
        "    - predictions (torch.Tensor): Model predictions (logits or probabilities) [batch_size, num_classes, H, W].\n",
        "    - threshold (float): Confidence threshold for determining confident pixels.\n",
        "\n",
        "    Returns:\n",
        "    - lambda_value (float): Adaptive lambda for the current batch.\n",
        "    \"\"\"\n",
        "    # Convert logits to probabilities (if necessary)\n",
        "    probs = F.softmax(predictions, dim=1)  # [batch_size, num_classes, H, W]\n",
        "\n",
        "    # Get maximum probabilities for each pixel\n",
        "    max_probs, _ = probs.max(dim=1)  # [batch_size, H, W]\n",
        "\n",
        "    # Create a mask of confident pixels\n",
        "    confident_mask = max_probs > threshold  # [batch_size, H, W]\n",
        "\n",
        "    # Calculate the proportion of confident pixels for each image in the batch\n",
        "    confident_proportions = confident_mask.float().mean(dim=(1, 2))  # [batch_size]\n",
        "\n",
        "    # Average over the batch to get the lambda value\n",
        "    lambda_value = confident_proportions.mean().item()\n",
        "\n",
        "    return lambda_value\n",
        "\n",
        "# DACS training loop\n",
        "def train_dacs_and_validate(model, source_loader, target_loader, num_epochs=20, lambda_param=0.7, lr=1e-4, confidence_threshold=0.7):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)  # Move model to device (CUDA or CPU)\n",
        "    model.train()\n",
        "    best_miou = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_miou = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Iterate through both source and target datasets for training\n",
        "        train_progress_bar = tqdm(zip(source_loader, target_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\", total=min(len(source_loader), len(target_loader)))\n",
        "        for (source_data, target_data) in train_progress_bar:\n",
        "\n",
        "            source_images, source_labels = source_data\n",
        "            target_images, _ = target_data  # Target labels are not available, will use pseudo-labels\n",
        "\n",
        "            # Move to device\n",
        "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
        "            target_images = target_images.to(device)\n",
        "\n",
        "            # 1. Forward pass through the model for the source and target domains\n",
        "            source_preds = model(source_images)\n",
        "            main_output_source= source_preds[1]\n",
        "            #print(\"main_output_source shape : \", main_output_source.shape)\n",
        "            main_output_source = F.interpolate(main_output_source, size=(512,512), mode='bilinear', align_corners=True)\n",
        "            #print(\"main_output_source shape : \", main_output_source.shape) #torch.Size([2, 7, 512, 512])\n",
        "            #print(\"logits\", main_output_source[0, :, 0, 0]) #example: tensor([-3.4636, -3.5338,  2.0996, -0.4825, -0.7998,  1.7863,  2.0860], device='cuda:0', grad_fn=<SelectBackward0>)\n",
        "\n",
        "\n",
        "            # 2. Generate pseudo-label for target domain\n",
        "            target_preds = model(target_images)\n",
        "            main_output_target = target_preds[1]\n",
        "            #print(\"main_output_target shape : \", main_output_target.shape)\n",
        "            main_output_target = F.interpolate(main_output_target, size=(512,512), mode='bilinear', align_corners=True)\n",
        "            #print(\"main_output_target shape : \", main_output_target.shape) #torch.Size([2, 7, 512, 512])\n",
        "\n",
        "            #The goal of pseudo-labeling is to assign a single class label to each pixel based on the highest probability (or logit). The argmax function finds the index of the maximum value along the specified dimension (num_classes). The result tensor is (batch_size, height, width)\n",
        "            pseudo_labels = torch.argmax(main_output_target, dim=1).to(device)\n",
        "            #print(\"pseudo_labels\", pseudo_labels.shape) #torch.Size([2, 512, 512])\n",
        "            #print(\"source labels aaaaaa:\", source_labels) #here each class label is represented as a one-hot vector for each pixel across the image. we want convert the one-hot encoded source_labels into class indices, so each pixel gets assigned the class marked as 1 in the one-hot vector.\n",
        "            #If a pixel in source_labels has a one-hot encoded value like [1, 0, 0, 0], this means that the pixel belongs to class 0 (the first class). ->\n",
        "            source_labels = torch.argmax(source_labels, dim=1).to(device)\n",
        "            #print(\"source_labels\", source_labels.shape) # torch.Size([2, 512, 512])\n",
        "\n",
        "            # 3. Calculate adaptive lambda\n",
        "            #lambda_param = calculate_lambda(main_output_target, threshold=confidence_threshold)\n",
        "            #train_progress_bar.set_postfix({'lambda': lambda_param})\n",
        "\n",
        "            # 4. MIX source and target images and labels (DACS augmentation step)\n",
        "            mixed_images, mixed_labels = classmix(source_images, source_labels, target_images, pseudo_labels)\n",
        "            #print(\"mixed_labels \", mixed_labels.shape)\n",
        "            mixed_labels=mixed_labels.to(device)\n",
        "            #print(\"mixed_labels\", mixed_labels)\n",
        "\n",
        "            # 5. Forward pass for mixed data\n",
        "            mixed_preds = model(mixed_images)\n",
        "            main_output_mixed = mixed_preds[1]\n",
        "            #print(\"main_output_mixed shape : \", main_output_mixed.shape)\n",
        "            main_output_mixed = F.interpolate(main_output_mixed, size=(512,512), mode='bilinear', align_corners=True)\n",
        "            #print(\"main_output_mixed shape : \", main_output_mixed.shape) #torch.Size([2, 7, 512, 512])\n",
        "            #print(\"main_output_mixed\", main_output_mixed)\n",
        "\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = dacs_loss(main_output_source, source_labels, main_output_mixed, mixed_labels, lambda_param)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print loss and mIoU after each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_miou = 0.0\n",
        "        val_class_iou = torch.zeros(len(LABEL_MAP)).to('cuda')  # Store IoU per class\n",
        "        val_class_counts = torch.zeros(len(LABEL_MAP)).to('cuda')  # To track number of pixels for each class\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in target_loader:\n",
        "                images, masks = images.to('cuda'), masks.to('cuda')\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)  # [batch_size, num_classes, 512, 512]\n",
        "                main_output = outputs[1]\n",
        "                main_output = F.interpolate(main_output, size=(512,512), mode='bilinear', align_corners=True)\n",
        "                preds = torch.argmax(main_output, dim=1)  # [batch_size, 512, 512]\n",
        "\n",
        "                # Convert masks to class indices format if one-hot encoded\n",
        "                if masks.ndim == 4:  # [batch_size, num_classes, height, width]\n",
        "                    masks = masks.argmax(dim=1)  # [batch_size, height, width]\n",
        "\n",
        "                # Calculate per-class IoU\n",
        "                for c, class_name in LABEL_MAP.items():  # Iterate over LABEL_MAP classes\n",
        "                    true_class = (masks == class_name)\n",
        "                    pred_class = (preds == class_name)\n",
        "\n",
        "                    intersection = torch.sum(true_class & pred_class).float()\n",
        "                    union = torch.sum(true_class | pred_class).float()\n",
        "\n",
        "                    if union != 0:\n",
        "                        val_class_iou[class_name] += intersection / union\n",
        "                    val_class_counts[class_name] += 1\n",
        "\n",
        "                # Calculate overall mIoU for this batch\n",
        "                val_miou += jaccard(preds, masks)\n",
        "\n",
        "        # Average metrics\n",
        "        val_miou /= len(target_loader)\n",
        "\n",
        "        # Calculate average IoU for each class\n",
        "        avg_class_iou = val_class_iou / val_class_counts\n",
        "\n",
        "        # Print validation metrics at the end of each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation mIoU: {val_miou:.4f}\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Per-Class IoU: {dict(zip(LABEL_MAP.keys(), avg_class_iou.tolist()))}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_miou > best_miou:\n",
        "            best_miou = val_miou\n",
        "            torch.save(model.state_dict(), 'best_pidnet_model.pth')\n",
        "\n",
        "\n",
        "\n",
        "# Loading datasets\n",
        "source_dataset = LoveDADataset(root_dir='Train/', split='Train', transform_image=transform_image, transform_label=transform_label, subdir='Urban', aug1=AUG1, aug2=AUG2)\n",
        "target_dataset = LoveDADataset(root_dir='Val/', split='Val', transform_image=transform_image, transform_label=transform_label, subdir='Rural')\n",
        "\n",
        "source_loader = DataLoader(source_dataset, batch_size=2, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=PERSISTENT_WORKERS)\n",
        "target_loader = DataLoader(target_dataset, batch_size=2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "\n",
        "# Define the size of the subset (e.g., 10% of the dataset)\n",
        "# subset_size = 0.2  # 10% of the dataset\n",
        "# source_indices = list(range(len(source_dataset)))\n",
        "# target_indices = list(range(len(target_dataset)))\n",
        "\n",
        "# # Select a subset of indices\n",
        "# source_subset_indices = random.sample(source_indices, int(len(source_indices) * subset_size))\n",
        "# target_subset_indices = random.sample(target_indices, int(len(target_indices) * subset_size))\n",
        "\n",
        "# # Create subset datasets\n",
        "# source_subset = Subset(source_dataset, source_subset_indices)\n",
        "# target_subset = Subset(target_dataset, target_subset_indices)\n",
        "\n",
        "# # Create data loaders for the subset datasets\n",
        "# source_loader = DataLoader(source_subset, batch_size=2, shuffle=True)\n",
        "# target_loader = DataLoader(target_subset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "# Initialize, train the model and validate\n",
        "model = DACSModel(num_classes=7).to(device)  # Assuming 7 classes\n",
        "train_dacs_and_validate(model, source_loader, target_loader, num_epochs=20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vloYGWlwZmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1882d07b-7026-4d79-b591-167b46e8af69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-34d80a4c5901>:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n",
            "Epoch 1/20: 100%|██████████| 496/496 [03:17<00:00,  2.52batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 1.8365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Validation mIoU: 0.1039\n",
            "Epoch [1/20] - Per-Class IoU: {'Background': 0.37344419956207275, 'Building': 0.1255132555961609, 'Road': 0.10872494429349899, 'Water': 0.06966084241867065, 'Barren': 0.016518406569957733, 'Forest': 0.03077808953821659, 'Agricultural': 0.002549270633608103}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 496/496 [03:12<00:00,  2.58batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20] - Loss: 1.1617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20] - Validation mIoU: 0.1687\n",
            "Epoch [2/20] - Per-Class IoU: {'Background': 0.47125881910324097, 'Building': 0.17302414774894714, 'Road': 0.1273420751094818, 'Water': 0.32381367683410645, 'Barren': 0.036557335406541824, 'Forest': 0.02752094902098179, 'Agricultural': 1.5632480199201382e-07}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 496/496 [03:09<00:00,  2.61batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20] - Loss: 1.3195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20] - Validation mIoU: 0.1647\n",
            "Epoch [3/20] - Per-Class IoU: {'Background': 0.45952582359313965, 'Building': 0.16982294619083405, 'Road': 0.1258792281150818, 'Water': 0.3522223234176636, 'Barren': 0.00021935052063781768, 'Forest': 0.025669973343610764, 'Agricultural': 0.00015151375555433333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 496/496 [03:10<00:00,  2.61batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20] - Loss: 1.2047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20] - Validation mIoU: 0.2070\n",
            "Epoch [4/20] - Per-Class IoU: {'Background': 0.4981767535209656, 'Building': 0.14188852906227112, 'Road': 0.14088763296604156, 'Water': 0.37209805846214294, 'Barren': 0.03895910456776619, 'Forest': 0.02462450973689556, 'Agricultural': 0.22148659825325012}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 496/496 [03:06<00:00,  2.66batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20] - Loss: 1.1123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20] - Validation mIoU: 0.1977\n",
            "Epoch [5/20] - Per-Class IoU: {'Background': 0.48606985807418823, 'Building': 0.10590069741010666, 'Road': 0.13354773819446564, 'Water': 0.4687541127204895, 'Barren': 0.021110491827130318, 'Forest': 0.02725100889801979, 'Agricultural': 0.11873187869787216}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 496/496 [03:14<00:00,  2.55batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20] - Loss: 1.1715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20] - Validation mIoU: 0.1883\n",
            "Epoch [6/20] - Per-Class IoU: {'Background': 0.5057113766670227, 'Building': 0.10357625782489777, 'Road': 0.10390207171440125, 'Water': 0.4542142152786255, 'Barren': 0.013664841651916504, 'Forest': 0.011551674455404282, 'Agricultural': 0.1025133952498436}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 496/496 [03:14<00:00,  2.55batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20] - Loss: 1.2798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20] - Validation mIoU: 0.1411\n",
            "Epoch [7/20] - Per-Class IoU: {'Background': 0.12298566102981567, 'Building': 0.16180165112018585, 'Road': 0.14852923154830933, 'Water': 0.1915082186460495, 'Barren': 0.010012821294367313, 'Forest': 0.03211357444524765, 'Agricultural': 0.3150556981563568}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 496/496 [03:09<00:00,  2.62batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20] - Loss: 0.5734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20] - Validation mIoU: 0.1286\n",
            "Epoch [8/20] - Per-Class IoU: {'Background': 0.10040049999952316, 'Building': 0.13242334127426147, 'Road': 0.13794220983982086, 'Water': 0.18867823481559753, 'Barren': 0.005252309609204531, 'Forest': 0.02769097313284874, 'Agricultural': 0.30094265937805176}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 496/496 [03:11<00:00,  2.59batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20] - Loss: 1.1046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20] - Validation mIoU: 0.1360\n",
            "Epoch [9/20] - Per-Class IoU: {'Background': 0.10237661004066467, 'Building': 0.152702197432518, 'Road': 0.13623790442943573, 'Water': 0.18390126526355743, 'Barren': 0.02496633492410183, 'Forest': 0.019363457337021828, 'Agricultural': 0.3259127736091614}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 496/496 [03:10<00:00,  2.60batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20] - Loss: 1.0166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20] - Validation mIoU: 0.1403\n",
            "Epoch [10/20] - Per-Class IoU: {'Background': 0.09903305768966675, 'Building': 0.15567804872989655, 'Road': 0.14603111147880554, 'Water': 0.19114719331264496, 'Barren': 0.019640961661934853, 'Forest': 0.025811774656176567, 'Agricultural': 0.33750101923942566}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 496/496 [03:10<00:00,  2.60batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20] - Loss: 0.9760\n",
            "Epoch [11/20] - Validation mIoU: 0.1261\n",
            "Epoch [11/20] - Per-Class IoU: {'Background': 0.09961213916540146, 'Building': 0.1451573669910431, 'Road': 0.14095762372016907, 'Water': 0.17801977694034576, 'Barren': 0.06871029734611511, 'Forest': 0.01301886048167944, 'Agricultural': 0.23031648993492126}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 496/496 [03:13<00:00,  2.56batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20] - Loss: 0.9720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20] - Validation mIoU: 0.1342\n",
            "Epoch [12/20] - Per-Class IoU: {'Background': 0.10541581362485886, 'Building': 0.15484507381916046, 'Road': 0.142783060669899, 'Water': 0.18158750236034393, 'Barren': 0.00025643096887506545, 'Forest': 0.01816522888839245, 'Agricultural': 0.3295237123966217}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 496/496 [03:11<00:00,  2.60batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20] - Loss: 0.6272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20] - Validation mIoU: 0.1381\n",
            "Epoch [13/20] - Per-Class IoU: {'Background': 0.09924650937318802, 'Building': 0.154303178191185, 'Road': 0.13748528063297272, 'Water': 0.18992409110069275, 'Barren': 0.02827860601246357, 'Forest': 0.019551100209355354, 'Agricultural': 0.33179980516433716}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 496/496 [03:12<00:00,  2.58batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20] - Loss: 1.2091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20] - Validation mIoU: 0.1339\n",
            "Epoch [14/20] - Per-Class IoU: {'Background': 0.09412958472967148, 'Building': 0.14751411974430084, 'Road': 0.14508555829524994, 'Water': 0.18673402070999146, 'Barren': 0.003443701658397913, 'Forest': 0.028332361951470375, 'Agricultural': 0.32542574405670166}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 496/496 [03:09<00:00,  2.61batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20] - Loss: 0.7745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20] - Validation mIoU: 0.1405\n",
            "Epoch [15/20] - Per-Class IoU: {'Background': 0.09152840822935104, 'Building': 0.15404042601585388, 'Road': 0.14774854481220245, 'Water': 0.19506774842739105, 'Barren': 0.03251189365983009, 'Forest': 0.023713048547506332, 'Agricultural': 0.33227819204330444}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 496/496 [03:12<00:00,  2.58batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20] - Loss: 0.9039\n",
            "Epoch [16/20] - Validation mIoU: 0.1336\n",
            "Epoch [16/20] - Per-Class IoU: {'Background': 0.10051294416189194, 'Building': 0.1378137320280075, 'Road': 0.13952182233333588, 'Water': 0.18631716072559357, 'Barren': 0.012945245951414108, 'Forest': 0.018025638535618782, 'Agricultural': 0.33318352699279785}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 496/496 [03:13<00:00,  2.57batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20] - Loss: 0.6824\n",
            "Epoch [17/20] - Validation mIoU: 0.1358\n",
            "Epoch [17/20] - Per-Class IoU: {'Background': 0.08586490899324417, 'Building': 0.1501511037349701, 'Road': 0.15489701926708221, 'Water': 0.18877288699150085, 'Barren': 0.019028371199965477, 'Forest': 0.022871216759085655, 'Agricultural': 0.3222424387931824}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 496/496 [03:11<00:00,  2.58batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20] - Loss: 0.8830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20] - Validation mIoU: 0.1391\n",
            "Epoch [18/20] - Per-Class IoU: {'Background': 0.09168019890785217, 'Building': 0.1396915465593338, 'Road': 0.15485304594039917, 'Water': 0.20141734182834625, 'Barren': 0.03475085273385048, 'Forest': 0.03355035558342934, 'Agricultural': 0.3129860460758209}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 496/496 [03:12<00:00,  2.58batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20] - Loss: 0.7039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20] - Validation mIoU: 0.1404\n",
            "Epoch [19/20] - Per-Class IoU: {'Background': 0.10732676088809967, 'Building': 0.14357803761959076, 'Road': 0.1574588567018509, 'Water': 0.19725535809993744, 'Barren': 0.02062210813164711, 'Forest': 0.02178467996418476, 'Agricultural': 0.3286987245082855}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 496/496 [03:13<00:00,  2.57batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20] - Loss: 0.7704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20] - Validation mIoU: 0.1372\n",
            "Epoch [20/20] - Per-Class IoU: {'Background': 0.1027241200208664, 'Building': 0.14174510538578033, 'Road': 0.14628703892230988, 'Water': 0.20249570906162262, 'Barren': 0.013422747142612934, 'Forest': 0.027786964550614357, 'Agricultural': 0.3181988596916199}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze function in PyTorch is used to add an extra dimension to a tensor.\n",
        "# channels: 3 color channels (RGB)"
      ],
      "metadata": {
        "id": "vs-hJOV9TGK1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DACS method:\n",
        "1. ClassMix strategy:\n",
        "Half of the classes from a source domain image are selected, and the corresponding pixels are cut out and pasted onto an image from the target domain.\n",
        "The same mixing procedure is applied to the labels (pseudo-labels for the target domain and ground-truth labels for the source domain).\n",
        "2. Pseudo-labels:\n",
        "The target domain image is passed through the model before mixing to produce pseudo-labels. These labels are then mixed similarly to how the images are mixed.\n",
        "3. Cross-domain Mixing:\n",
        "By combining the two domains at the pixel level, this method attempts to make it harder for the model to learn to discern between the domains. This can mitigate the issue of class distribution differences between source and target domains.\n",
        "\n",
        "Key Steps for Implementation:\n",
        "1. Image Mixing:\n",
        "Select a portion of the source domain image and mix it with the target domain image.\n",
        "2. Label Mixing:\n",
        "Use the ground-truth label from the source domain and pseudo-labels from the target domain and mix them similarly to the images.\n",
        "3. Model Training:\n",
        "Once the mixed images and labels are prepared, the model can be trained using the usual supervised learning approach, where predictions are made and loss is computed using cross-entropy."
      ],
      "metadata": {
        "id": "wJLWYZ3YWgrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Training Phase:\n",
        "\n",
        "Input: Source images and source labels.\n",
        "\n",
        "Output: Predicted masks and loss between predictions and source labels.\n",
        "\n",
        "Augment with target data (via classmix).\n",
        "\n",
        "Backpropagation and optimization.\n",
        "\n",
        "2. Validation Phase:\n",
        "\n",
        "Input: Target images (no labels for target during training).\n",
        "\n",
        "Output: Predicted masks for the target images.\n",
        "\n",
        "Compute metrics such as mIoU between predicted masks and true target domain masks."
      ],
      "metadata": {
        "id": "vFR854e-56Xe"
      }
    }
  ]
}