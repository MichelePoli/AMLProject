\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{ragged2e}

\title{Real-time Domain Adaptation in Semantic Segmentation}
\author{Attrovio Mario, Ghisolfo Giorgia, Russo Michele}

\begin{document}
	\maketitle
	
	
	
	\begin{multicols}{2}
		
		
		
		\section{Abstract}
		Semantic segmentation is a critical task in computer vision, enabling pixel-wise classification of images. However, the performance of segmentation models often degrades when applied to data from different domains, a challenge known as domain shift. This report explores real-time semantic segmentation in the context of domain adaptation using PIDNet as the backbone. We investigate the performance drop caused by domain shift between urban and rural datasets and evaluate mitigation strategies, including data augmentation and advanced domain adaptation techniques like adversarial training and image-to-image translation (DACS). Experimental results on the LoveDA dataset demonstrate that these methods significantly reduce the impact of domain shift while maintaining real-time inference capabilities, achieving a balanced trade-off between accuracy and computational efficiency. The code can be found on our project website: \url{https://github.com/MichelePoli/AMLProject}.
		
		\section{Introduction}
		Semantic segmentation is a foundational task in computer vision, where each pixel in an image is assigned a label corresponding to a predefined class. It plays a vital role in applications such as autonomous driving, medical imaging, and remote sensing. Recent advancements in deep learning have yielded high-performing models, but these often struggle with domain shift—a phenomenon where a model trained on a source domain (e.g., urban images) performs poorly on a target domain (e.g., rural images) \cite{loveda2021}. Addressing this challenge is crucial for real-world deployments where annotated data for all target domains is scarce or unavailable.
		
		Domain adaptation aims to bridge this performance gap by aligning the source and target domains without requiring extensive labeled data from the target domain. While several methods exist, real-time semantic segmentation introduces additional constraints, such as maintaining high inference speed and low computational cost. PIDNet \cite{pidnet2023}, a real-time segmentation network inspired by Proportional-Integral-Derivative (PID) controllers, serves as the backbone for our study due to its efficiency and accuracy in real-time tasks.
		
		This report focuses on evaluating and improving the performance of PIDNet for domain-adaptive semantic segmentation using the LoveDA dataset \cite{loveda2021}. We first quantify the performance degradation caused by domain shift. Next, we implement data augmentation techniques and two domain adaptation approaches—adversarial training and image-to-image translation (DACS)—to mitigate this issue. Our findings highlight the potential of these methods to enhance generalization while preserving the real-time capabilities of the model.
		
		\section{Related Work}
		Semantic segmentation has evolved significantly with deep learning. Fully Convolutional Networks (FCNs) \cite{long2015fcn} pioneered end-to-end segmentation by replacing fully connected layers with convolutional ones, enabling the network to process images of arbitrary sizes and produce pixel-wise predictions. DeepLabV2 \cite{chen2018deeplab} introduced atrous convolutions, which expand the receptive field without increasing the number of parameters, allowing for multi-scale context aggregation. Real-time networks like BiSeNet \cite{yu2018bisenet} optimized speed-accuracy trade-offs using lightweight backbones and parallel structures, while PIDNet \cite{pidnet2023} further improved efficiency by mimicking PID controllers to balance high-, mid-, and low-level features.
		
		Domain adaptation techniques address performance degradation across domains. Adversarial methods \cite{tsai2018learning} train a discriminator to distinguish between source and target domain features, encouraging the feature extractor to produce domain-invariant representations. Image-to-image translation approaches like DACS \cite{tranheden2021dacs} blend domains through mixed sampling, generating pseudo-labeled target domain images to improve model generalization. The LoveDA dataset \cite{loveda2021} provided urban/rural splits to benchmark these methods in remote sensing, offering a challenging scenario for domain adaptation research.
		
		\section{Methods}
		\subsection{Baseline Training}
		We first trained a classic semantic segmentation network, DeepLabV2 \cite{chen2018deeplab}, on the LoveDA-urban dataset for 20 epochs using a ResNet-101 backbone \cite{he2016deep} pre-trained on ImageNet. DeepLabV2 employs atrous convolutions to capture multi-scale context, which is crucial for accurate segmentation. However, its high computational cost makes it less suitable for real-time applications. Then, we trained PIDNet-S \cite{pidnet2023}, also pre-trained on ImageNet, on LoveDA-urban for 20 epochs to establish an upper bound for real-time performance (mIoU: 48.67\%). PIDNet was chosen for its design that balances high-, mid-, and low-level features, similar to a PID controller, enabling efficient and accurate segmentation. Domain shift was quantified by testing this PIDNet-S model on LoveDA-rural (mIoU: 23.98\%).
		
		\subsection{Data Augmentation}
		Data augmentation was used to improve the model's generalization capability by increasing the diversity of the training data. Two augmentation strategies were applied during training with a probability of 0.5:
		\begin{itemize}
			\item \textbf{A1}: Geometric transforms (horizontal/vertical flips, 30° rotation)
			\item \textbf{A2}: Photometric transforms (ColorJitter, GaussianBlur)
		\end{itemize}
		The best single augmentation (A2) improved rural mIoU to 30.80\%, while combining A1 and A2 yielded 29.35\%. Photometric augmentations were more effective because they help the model become invariant to changes in lighting and color, which are common differences between the urban and rural domains.
		\subsection{Domain Adaptation}
		\subsubsection{Adversarial Training}
		Adversarial training aims to make the feature representations domain-invariant. A discriminator \cite{tsai2018learning} was trained against PIDNet's features using a binary cross-entropy loss with a $\lambda$ value of 0.0005. The learning rate for the discriminator was set to $5 \times 10^{-4}$. The discriminator's goal is to distinguish between features from the source and target domains, while the segmentation network tries to fool the discriminator. This approach achieved 30.59\% mIoU on the target domain. The limitation of this method is the difficulty in balancing the training of the discriminator and the segmentation network.
		
		\subsubsection{DACS}
		Domain Adaptation via Cross-domain Mixed Sampling (DACS) \cite{tranheden2021dacs} was implemented to blend classes between the source and target domains. DACS mixes images and labels from both domains at the class level, creating a new training set that encourages the model to learn features that are useful for both domains. This method reached 30.63\% mIoU on the target domain. DACS is particularly effective when the domains have significant differences in class distributions.
		
		\subsection{Extensions}
		\subsubsection{Style Transfer Preprocessing}
		We applied style transfer using a pre-trained model to preprocess the source domain images, attempting to match the target domain's appearance. This method adapts the visual style of the source images to that of the target images, reducing the domain gap. This resulted in an mIoU of 25.46\% on the target domain. While this method is intuitive, it can be sensitive to the choice of the style transfer model and may not capture all domain-specific characteristics.
		
		\subsubsection{Alternative Models}
		We explored two alternative real-time segmentation models:
		\begin{itemize}
			\item \textbf{BiSeNetV1} \cite{yu2018bisenet}: Achieved 32.21\% mIoU on the target domain. BiSeNetV1 uses a spatial path and a context path to capture spatial information and context dependencies, respectively.
			\item \textbf{LinkNet} \cite{chaurasia2017linknet}: Achieved 30.43\% mIoU on the target domain. LinkNet employs an encoder-decoder architecture with skip connections, which helps in preserving spatial information.
		\end{itemize}
		These models were chosen for their efficiency and different architectural designs, providing a comparison to PIDNet.
		
		\section{Experimental Results}
		\subsection{LoveDA Dataset}  
		The LoveDA dataset \cite{loveda2021} is a high-resolution (0.3 m) remote sensing dataset designed for domain-adaptive semantic segmentation in urban and rural environments. It contains 5,987 images across three Chinese cities (Nanjing, Changzhou, Wuhan), annotated with seven classes: *background*, *building*, *road*, *water*, *barren*, *forest*, and *agriculture*. The dataset is explicitly divided into urban (2,522 images) and rural (3,465 images) domains to study domain shift challenges.  
		
		Key characteristics of LoveDA include:  
		\begin{itemize}  
			\item \textbf{Multi-scale Objects}: Urban scenes feature densely packed buildings and structured roads, while rural areas contain scattered agricultural plots and irregular water bodies. Buildings in urban regions exhibit larger size variance compared to rural regions (Figure \ref{fig:scale}).  
			\item \textbf{Complex Backgrounds}: The *background* class dominates both domains (Figure \ref{fig:class_dist}), encompassing diverse elements like vehicles and undeveloped land, which introduces high intra-class variance.  
			\item \textbf{Domain Shift}: Urban and rural domains exhibit divergent class distributions (e.g., urban has 32\% buildings vs. rural’s 8\%) and spectral properties (lower variance in rural areas due to homogeneous landscapes).  
		\end{itemize}  
		
		\begin{figure}[ht]  
			\centering  

			\caption{Class distribution differences between urban and rural domains in LoveDA. Buildings dominate urban areas, while agriculture is prevalent in rural regions.}  
			\label{fig:class_dist}  
		\end{figure}  
		
		For domain adaptation experiments, LoveDA provides two tasks:  
		\begin{enumerate}  
			\item \textbf{Urban $\rightarrow$ Rural}: Train on urban data (Qinhuai, Qixia, Jianghan, Gulou) and test on rural (Jiangning, Xinbei, Liyang).  
			\item \textbf{Rural $\rightarrow$ Urban}: Train on rural data (Pukou, Lishui, Gaochun, Jiangxia) and test on urban (Jiangye, Wuchang, Wujin).  
		\end{enumerate}  
		
		The dataset’s inherent challenges—scale variation, background complexity, and domain-specific class imbalances—make it a rigorous benchmark for evaluating real-time domain adaptation methods.  
		
		\begin{figure}[ht]  
			\centering  

			\caption{Scale differences in building sizes between urban (Jianye) and rural (Lishui) regions. Urban buildings exhibit greater size variability.}  
			\label{fig:scale}  
		\end{figure}
		\subsection{Performance on Source Domain}
		Table \ref{tab:urban} shows the performance of DeepLabV2 and PIDNet-S on the LoveDA-urban dataset (source domain). PIDNet-S, designed for real-time performance, achieved a significantly higher mIoU than DeepLabV2 while also providing latency, FLOPs, and parameter count.
		
			\vspace{2cm}
			\centering
			\caption{Performance on LoveDA-Urban (Source Domain)}
			\label{tab:urban}
			\tiny
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				Model & mIoU (\%) & Latency (ms) & FLOPs & Params \\ \hline
				DeepLabV2 & 34.40 & - & - & - \\ \hline
				PIDNet-S & 48.67 & 288.70 & 6.35G & 7.72M \\ \hline
			\end{tabular}
				\vspace{0.2cm}
		
		
		
		

		
		\subsection{Domain Shift Evaluation}
		\small
		Table \ref{tab:shift} quantifies the domain shift from LoveDA-urban to LoveDA-rural. The baseline PIDNet-S model, trained on urban data, experienced a significant performance drop when tested on rural data. Data augmentations (A1 and A2) improved the mIoU, with A2 (photometric transforms) being the most effective. Adversarial training and DACS further mitigated the domain shift, achieving similar performance.
		

			\centering
			\caption{Domain Shift: Urban $\rightarrow$ Rural}
			\label{tab:shift}
	 		\resizebox{0.5\textwidth}{!}{%
			\begin{tabular}{|l|c|c|c|c|c|c|c|}
				\hline
				Model & Road & Building & Water & Barren & Forest & Agric. & mIoU \\ \hline
				PIDNet & 16.34 & 23.99 & 35.46 & 3.12 & 8.83 & 31.82 & 23.98 \\ \hline
				+ A1 & 29.33 & 40.77 & 36.71 & 9.17 & 9.53 & 33.13 & 29.91 \\ \hline
				+ A2 & 31.41 & 38.37 & 31.33 & 10.26 & 15.10 & 37.51 & 30.80 \\ \hline
				+ A1 + A2 & 27.92 & 32.97 & 33.98 & 10.50 & 10.69 & 37.45 & 29.35 \\ \hline
				+ Adv. & 0.36 & 13.41 & 32.72 & 8.28 & 49.73 & 12.26 & 30.59 \\ \hline
				+ DACS & 31.32 & 36.60 & 42.70 & 4.49 & 2.70 & 40.60 & 30.63 \\ \hline
			\end{tabular}%
		}

		
		\subsection{Extensions}
		\justifying
		Table \ref{tab:extensions} presents the results of the extensions on the LoveDA-rural dataset. Style transfer preprocessing improved the mIoU slightly compared to the baseline but was less effective than data augmentation or domain adaptation techniques. BiSeNetV1 and LinkNet showed competitive performance, with BiSeNetV1 outperforming PIDNet-S on the target domain.
		

			\centering
			\caption{Extensions on LoveDA-Rural (Target Domain)}
			\label{tab:extensions}
			\begin{tabular}{|l|c|}
				\hline
				Model & mIoU (\%) \\ \hline
				PIDNet + Style Transfer & 25.46 \\ \hline
				BiSeNetV1 & 32.21 \\ \hline
				LinkNet & 30.43 \\ \hline
			\end{tabular}

		\subsection{Comparison with Original UDA Paper Results}
		Table \ref{tab:uda_comparison} compares our domain adaptation results with those reported in the original DACS paper \cite{tranheden2021dacs} and LoveDA benchmarks \cite{loveda2021}. While DACS achieved 39.10\% mIoU on LoveDA-rural in the original implementation, our PIDNet-S adaptation reached only 30.63\%. Similarly, adversarial training underperformed compared to Tsai et al. \cite{tsai2018learning} (30.59\% vs. 35.20\%). Three key factors explain these differences:
		
		\begin{itemize}
			\item \textbf{Model Architecture}: The original DACS paper used DeepLabV2 with ResNet-101, which has significantly higher capacity (44.5M params) compared to our real-time PIDNet-S (7.72M params). This architectural difference directly impacts feature representation power.
			\item \textbf{Training Constraints}: Our experiments used a fixed 20-epoch training schedule to maintain real-time deployment capabilities, whereas the original works employed longer training (50+ epochs) with extensive hyperparameter tuning.
			\item \textbf{Latency-Accuracy Trade-off}: PIDNet-S prioritizes inference speed (288ms) over pure accuracy, while DeepLabV2-based implementations ignore latency constraints (typically >1,000ms).
		\end{itemize}
		
		\justifying
		These results highlight the inherent challenge of balancing domain adaptation performance with real-time requirements—a critical consideration for edge deployment scenarios.
		

			\centering
			\caption{Comparison with Original UDA Paper Results (LoveDA-Rural)}
			\label{tab:uda_comparison}
			\resizebox{0.5\textwidth}{!}{%
			\begin{tabular}{|l|l|c|c|}
				\hline
				Method & Model & mIoU (\%) & Latency (ms) \\ \hline
				DACS (Original) \cite{tranheden2021dacs} & DeepLabV2 + ResNet-101 & 39.10 & 1,200 \\ \hline
				DACS (Ours) & PIDNet-S & 30.63 & 288 \\ \hline
				Adv. Training (Original) \cite{tsai2018learning} & DeepLabV2 + VGG16 & 35.20 & 850 \\ \hline
				Adv. Training (Ours) & PIDNet-S & 30.59 & 288 \\ \hline
			\end{tabular}%
		}

		\section{Conclusion}
		\justifying
		This work demonstrates PIDNet's effectiveness for real-time semantic segmentation on the LoveDA-urban dataset (288ms latency, 48.67\% mIoU) and quantifies the domain shift effect when applied to LoveDA-rural (23.98\% mIoU). Data augmentation, particularly photometric transforms (A2), provided the most consistent improvement (+6.82\% mIoU), while adversarial training and DACS showed moderate gains (+6.61\% and +6.65\% mIoU, respectively). Extensions with alternative models highlighted PIDNet's superior speed-accuracy balance, although BiSeNetV1 performed slightly better on the target domain. Future work could explore hybrid adaptation strategies, optimized style transfer pipelines, and the combination of DACS and adversarial training for further performance enhancements.
		
	\end{multicols}
	
	\bibliographystyle{ieeetran}
	\bibliography{references}
	
\end{document}