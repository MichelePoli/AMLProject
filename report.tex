\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{graphicx}


\title{Real-time Domain Adaptation in Semantic Segmentation}
\author{Attrovio Mario, Ghisolfo Giorgia, Russo Michele}

\begin{document}
	\maketitle
	
	
	
	\begin{multicols}{2}
		
		
		
		\section{Abstract}
		Semantic segmentation is a critical task in computer vision, enabling pixel-wise classification of images. However, the performance of segmentation models often degrades when applied to data from different domains, a challenge known as domain shift. This report explores real-time semantic segmentation in the context of domain adaptation using PIDNet as the backbone. We investigate the performance drop caused by domain shift between urban and rural datasets and evaluate mitigation strategies, including data augmentation and advanced domain adaptation techniques like adversarial training and image-to-image translation (DACS)\cite{tranheden2021dacs}. Experimental results on the LoveDA dataset \cite{loveda2021} demonstrate that these methods reduce the impact of domain shift while maintaining real-time inference capabilities, achieving a balanced trade-off between accuracy and computational efficiency. The code can be found on our project website: \url{https://github.com/MichelePoli/AMLProject}.
		
		\section{Introduction}
		Semantic segmentation is a foundational task in computer vision, where each pixel in an image is assigned a label corresponding to a predefined class. It plays a vital role in applications such as autonomous driving, medical imaging, and remote sensing. Recent advancements in deep learning have yielded high-performing models, but these often struggle with domain shift—a phenomenon where a model trained on a source domain (e.g., urban images) performs poorly on a target domain (e.g., rural images) \cite{loveda2021}. Addressing this challenge is crucial for real-world deployments where annotated data for all target domains is scarce or unavailable.
		
		Domain adaptation aims to bridge this performance gap by aligning the source and target domains without requiring extensive labeled data from the target domain. While several methods exist, real-time semantic segmentation introduces additional constraints, such as maintaining high inference speed and low computational cost. PIDNet \cite{pidnet2023}, a real-time segmentation network inspired by Proportional-Integral-Derivative (PID) controllers, serves as the backbone for our study due to its efficiency and accuracy in real-time tasks.
		
		This report focuses on evaluating and improving the performance of PIDNet for domain-adaptive semantic segmentation using the LoveDA dataset \cite{loveda2021}. We first quantify the performance degradation caused by domain shift. Next, we implement data augmentation techniques and two domain adaptation approaches—adversarial training and image-to-image translation (DACS)\cite{tranheden2021dacs} —to mitigate this issue. Our findings highlight the potential of these methods to enhance generalization while preserving the real-time capabilities of the model.
		
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.8\linewidth]{image/urban_1.png}
			\caption{Una descrizione dell'immagine.}
			\label{fig:urban_image}
		  \end{figure}





		\section{Related Work}
		Semantic segmentation has evolved significantly with deep learning. 
		\\
		\\
		\textbf{Foundational Architectures}
		Semantic segmentation's deep learning era began with the introduction of Fully Convolutional Networks (FCNs) \cite{long2015fcn}. By replacing fully connected layers with convolutional ones, FCNs enabled end-to-end segmentation, capable of handling images of arbitrary sizes and providing pixel-wise predictions. This innovation laid the groundwork for modern segmentation models.
		Further advancements were introduced with DeepLabV2 \cite{chen2018deeplab}, which leveraged atrous (or dilated) convolutions. Atrous convolutions expanded the receptive field without increasing the number of parameters, making it possible to aggregate multi-scale contextual information efficiently.
		\\
		\\
		\textbf{Real-Time Efficiency}
		Real-time semantic segmentation requires optimizing the balance between accuracy and computational efficiency. BiSeNet \cite{yu2018bisenet} introduced lightweight backbones and parallel structures, achieving an effective speed-accuracy trade-off. This architecture focused on enhancing real-time processing capabilities for applications with strict latency requirements. PIDNet \cite{pidnet2023} further pushed the boundaries of efficiency by adopting principles from PID (Proportional-Integral-Derivative) controllers. The model effectively balanced high-, mid-, and low-level features, offering improvements in both speed and segmentation quality.
		\\
		\\
		\textbf{Domain Adaptation Techniques}
		Semantic segmentation models often suffer performance degradation when applied across different domains (e.g., urban vs. rural settings). To address this, various domain adaptation techniques have been developed two main method. Adversarial Methods \cite{tsai2018learning}: these approaches train a discriminator to differentiate between source and target domain features. By doing so, the feature extractor is encouraged to generate domain-invariant representations, improving generalization. Image-to-Image Translation: Methods like DACS \cite{tranheden2021dacs} blend domains by leveraging mixed sampling techniques. This strategy generates pseudo-labeled target domain images, enhancing the model's ability to generalize across domains.
		Additionally, the introduction of the LoveDA dataset \cite{loveda2021} provided a benchmark specifically designed for domain adaptation in remote sensing. With urban and rural splits, LoveDA poses a challenging scenario, advancing research in this field.

		
		
\section{Methods}
\subsection{Baseline Model Development}
\subsubsection{2a Classic Segmentation Architecture } 
We implemented DeepLabV2  \cite{chen2018deeplab}  with ResNet-101 backbone, pre-trained on ImageNet, using the LoveDA-urban dataset for training. This architecture leverages atrous spatial pyramid pooling to capture multi-scale contextual information through dilated convolutions. While effective for dense prediction tasks, its computational complexity makes it suboptimal for latency-sensitive applications. The model was initialized with ImageNet weights and trained and evaluated on urban scenes.

\subsubsection{2b Efficient Segmentation Framework}
To establish real-time capability baselines, we employed PIDNet-S - a streamlined architecture that mimics proportional-integral-derivative control mechanisms through three parallel branches. These branches explicitly manage high-frequency details, mid-level structures, and low-level spatial relationships respectively. The model was similarly initialized with ImageNet weights and trained and evaluated on urban scenes.
\subsubsection{PIDNet Architecture Details}
The core PIDNet architecture \cite{pidnet2023} consists of three branches
\begin{itemize}
	\item \textbf{Proportional (P) Branch}: Processes full-resolution inputs using shallow layers to preserve spatial precision. Contains edge-aware convolutions for sharp boundary detection.
	\item \textbf{Integral (I) Branch}: Leverages deep layers with dilated convolutions and spatial pooling to capture multi-scale context. Integrates a Semantic Guidance Module (SGM) to filter low-level noise.
	\item \textbf{Derivative (D) Branch}: Computes high-frequency feature discrepancies between adjacent stages using depthwise separable convolutions. Acts as a boundary corrector by amplifying transitional regions.
\end{itemize}
The branches are fused through a \textit{Three-Way Attention Fusion (TWAF)} module that dynamically combines features using spatial and channel attention weights.
 
\subsection{3a Domain Adaptation }
\label{subsec:domain_shift}
We investigated cross-domain generalization by evaluating our urban-trained PIDNet-S on the LoveDA-rural dataset without fine-tuning. This experimental design isolates the domain shift problem between man-made urban environments (characterized by geometric regularity and dense infrastructure) and rural landscapes (featuring organic shapes, sparse structures, and varied terrain).
\\
\subsubsection{Domain Adaptation challenges}
The performance degradation observed could be caused by:  
\begin{itemize}
\item1) \textbf{Object Scale Variance}: Urban structures maintain relatively consistent scales, while rural elements exhibit greater size variability (trees, water bodies)  
\item2) \textbf{Contextual Dependencies}: Urban scene semantics rely on positional relationships (e.g., roads between buildings), whereas rural contexts depend more on texture and color patterns  
\item3) \textbf{Surface Color Distribution}: Artificial materials dominate urban areas (concrete, glass), contrasting with natural materials (soil, vegetation) prevalent in rural regions
\end{itemize}
This analysis motivates subsequent domain adaptation strategies to bridge the feature distribution gap between structurally distinct environments.


		\subsection{3b Data Augmentation}
		Data augmentation was used to improve the model's generalization capability by increasing the diversity of the training data. Two augmentation strategies were applied during training with a probability of 0.5:
		\begin{itemize}
			\item \textbf{A1}: Geometric transforms (horizontal/vertical flips, 30° rotation)
			\item \textbf{A2}: Photometric transforms (ColorJitter, GaussianBlur)
		\end{itemize}
		Photometric augmentations were more effective because they help the model become invariant to changes in lighting and color, which are common differences between the urban and rural domains.
		
\subsection{Unsupervised Domain Adaptation}  
 
This method aligns domains through adversarial training on segmentation outputs \cite{tsai2018learning}, comprising three phases:  

\paragraph{Phase 1: Source Domain training}  
The segmentation network $G$ processes source images $I_s$ to produce softmax probability maps:  
\begin{equation}  
	P_s = {softmax}(G(I_s)) \in {R}^{C \times W \times H}  
\end{equation}  
where $C$ denotes semantic classes. Training uses standard cross-entropy loss with source labels $Y_s$.  



\paragraph{Phase 2: Adversarial Alignment}  
The segmentation network now receives gradients to confuse $D$ by:  
\begin{itemize}  
	\item Maximizing discriminator uncertainty on $P_t$ (domain label 1)
	\item Enforcing similar output distributions $P_s \approx P_t$  

\end{itemize}  

\paragraph{Phase 3: Discriminator Training}  
A fully-convolutional discriminator $D$ learns to distinguish:  
\begin{itemize}  
	\item Source softmax outputs $P_s$ (domain label 1)  
	\item Target softmax outputs $P_t = {softmax}(G(I_t))$ (domain label 0)  
\end{itemize}  


This single-stage approach achieves domain invariance by directly matching the structured output distributions rather than intermediate features.  

	\subsubsection{4b DACS}
	
	Domain Adaptation via Cross-domain Mixed Sampling (DACS) \cite{tranheden2021dacs} addresses unsupervised domain adaptation (UDA) by blending data from the source and target domains through class-level mixing. The main aspects of DACS are summarized below:
	
	\begin{itemize}
		\item \textbf{Augmented Training Samples:} 
		DACS creates hybrid samples by combining:
		\begin{itemize}
			\item A source domain image, which comes with ground-truth labels.
			\item A target domain image, which has pseudo-labels generated by the model.
		\end{itemize}
		During this process, a subset of classes is selected from the source image using its semantic map, and the corresponding pixels are pasted onto the target image.
		
		\item \textbf{Composite Label Generation:}
		The labels for the mixed image are produced by merging:
		\begin{itemize}
			\item Ground-truth labels from the source image.
			\item Pseudo-labels predicted for the target image.
		\end{itemize}
		This ensures that the model is trained on hybrid data containing both reliable source annotations and target pseudo-labels, encouraging feature invariance across domains.
		

		\item \textbf{Training Loss:} 
		The overall loss combines:
		\begin{itemize}
			\item Supervised learning on the source domain data.
			\item Consistency regularization on the mixed-domain samples.
		\end{itemize}
		Importantly, this does not require any annotations from the target domain.
		

	\end{itemize}
	
		
		\subsubsection{Extensions}
		We explored three alternative real-time semantic segmentation architectures:
		\begin{itemize}
			\item \textbf{BiSeNetV1} \cite{yu2018bisenet}: Employs a dual-path design with a \textit{spatial path} (high-resolution stream for fine details) and a \textit{context path} (fast downsampling with global average pooling for long-range dependencies). The features are fused using a specialized Feature Fusion Module (FFM).
			\item \textbf{LinkNet} \cite{chaurasia2017linknet}: Uses a lightweight encoder-decoder structure with residual skip connections. The encoder reduces spatial dimensions through cascaded convolutional blocks, while the decoder employs transposed convolutions for upsampling, with direct additive links between corresponding encoder-decoder layers.
			\item \textbf{STDC n} \cite{fan2021rethinking}: Features a Short-Term Dense Concatenate backbone that progressively aggregates multi-scale features through dense connections in early stages, followed by a Context Path with Attention Refinement Modules (ARMs) to enhance contextual awareness.
		\end{itemize}
		These models emphasize distinct architectural strategies: BiSeNetV1's parallel multi-resolution processing, LinkNet's symmetrical skip connections, and STDCNet's dense feature aggregation.
		
		
		
		\section{Experimental Results}
		\subsection{LoveDA Dataset}  
		The LoveDA dataset \cite{loveda2021} is a high-resolution (0.3 m) remote sensing dataset designed for domain-adaptive semantic segmentation in urban and rural environments. It contains 5,987 images across three Chinese cities (Nanjing, Changzhou, Wuhan), annotated with seven classes: *background*, *building*, *road*, *water*, *barren*, *forest*, and *agriculture*. The dataset is explicitly divided into urban (2,522 images) and rural (3,465 images) domains to study domain shift challenges.  
		
		Key characteristics of LoveDA include:  
		\begin{itemize}  
			\item \textbf{Multi-scale Objects}: Urban scenes feature densely packed buildings and structured roads, while rural areas contain scattered agricultural plots and irregular water bodies. Buildings in urban regions exhibit larger size variance compared to rural regions (Figure \ref{fig:scale}).  
			\item \textbf{Complex Backgrounds}: The *background* class dominates both domains (Figure \ref{fig:class_dist}), encompassing diverse elements like vehicles and undeveloped land, which introduces high intra-class variance.  
			\item \textbf{Domain Shift}: Urban and rural domains exhibit divergent class distributions (e.g., urban has 32\% buildings vs. rural’s 8\%) and spectral properties (lower variance in rural areas due to homogeneous landscapes).  
		\end{itemize}  
		
		\begin{figure}[ht]  
			\begin{center}

			\caption{Class distribution differences between urban and rural domains in LoveDA. Buildings dominate urban areas, while agriculture is prevalent in rural regions.}  
			\label{fig:class_dist} 
			\end{center} 

		\end{figure}  
		
		For domain adaptation experiments, LoveDA provides two tasks:  
		\begin{enumerate}  
			\item \textbf{Urban $\rightarrow$ Rural}: Train on urban data (Qinhuai, Qixia, Jianghan, Gulou) and test on rural (Jiangning, Xinbei, Liyang).  
			\item \textbf{Rural $\rightarrow$ Urban}: Train on rural data (Pukou, Lishui, Gaochun, Jiangxia) and test on urban (Jiangye, Wuchang, Wujin).  
		\end{enumerate}  
		
		The dataset’s inherent challenges—scale variation, background complexity, and domain-specific class imbalances—make it a rigorous benchmark for evaluating real-time domain adaptation methods.  
		
		\begin{figure}[ht]  
			\begin{center} 

			\caption{Scale differences in building sizes between urban (Jianye) and rural (Lishui) regions. Urban buildings exhibit greater size variability.}  
			\label{fig:scale}  
			\end{center} 
		\end{figure}
		\subsection{Performance on Source Domain}
		Table \ref{tab:urban} shows the performance of DeepLabV2 and PIDNet-S on the LoveDA-urban dataset (source domain). PIDNet-S, designed for real-time performance, achieved a significantly higher mIoU than DeepLabV2 while also providing latency, FLOPs, and parameter count.
		\begin{center} 
			\vspace{2cm}
			
				% \caption{Performance on LoveDA-Urban (Source Domain)}
			\label{tab:urban}
			
			\tiny
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				Model & mIoU (\%) & Latency (ms) & FLOPs & Params \\ \hline
				DeepLabV2 & 34.40 & - & - & - \\ \hline
				PIDNet-S & 48.67 & 288.70 & 6.35G & 7.72M \\ \hline
			\end{tabular}
				\vspace{0.2cm}
		\end{center} 
		
		
		
		

		
		\subsection{Domain Shift Evaluation}
		\small
		Table \ref{tab:shift} quantifies the domain shift from LoveDA-urban to LoveDA-rural. The baseline PIDNet-S model, trained on urban data, experienced a significant performance drop when tested on rural data. Data augmentations (A1 and A2) improved the mIoU, with A2 (photometric transforms) being the most effective. Adversarial training and DACS further mitigated the domain shift, achieving similar performance.
		\begin{center}

			% \caption{Domain Shift: Urban $\rightarrow$ Rural}
			\label{tab:shift}
		

	 		\resizebox{0.5\textwidth}{!}{%
			\begin{tabular}{|l|c|c|c|c|c|c|c|}
						\hline
						Model & Road & Building & Water & Barren & Forest & Agric. & mIoU \\ \hline
						PIDNet & 16.34 & 23.99 & 35.46 & 3.12 & 8.83 & 31.82 & 23.98 \\ \hline
						+ A1 & 29.33 & 40.77 & 36.71 & 9.17 & 9.53 & 33.13 & 29.91 \\ \hline
						+ A2 & 31.41 & 38.37 & 31.33 & 10.26 & 15.10 & 37.51 & 30.80 \\ \hline
						+ A1 + A2 & 27.92 & 32.97 & 33.98 & 10.50 & 10.69 & 37.45 & 29.35 \\ \hline
						+ Adv. & 0.36 & 13.41 & 32.72 & 8.28 & 49.73 & 12.26 & 30.59 \\ \hline
						+ DACS & 31.32 & 36.60 & 42.70 & 4.49 & 2.70 & 40.60 & 30.63 \\ \hline
					\end{tabular}%
				}
		\end{center} 

		
		\subsection{Extensions}
		\justifying
		Table \ref{tab:extensions} presents the results of the extensions on the LoveDA-rural dataset. Style transfer preprocessing improved the mIoU slightly compared to the baseline but was less effective than data augmentation or domain adaptation techniques. BiSeNetV1 and LinkNet showed competitive performance, with BiSeNetV1 outperforming PIDNet-S on the target domain.
		\begin{center}

			% \centering
			% \caption{Extensions on LoveDA-Rural (Target Domain)}
			\label{tab:extensions}
			\begin{tabular}{|l|c|}
				\hline
				Model & mIoU (\%) \\ \hline
				PIDNet + Style Transfer & 25.46 \\ \hline
				BiSeNetV1 & 32.21 \\ \hline
				LinkNet & 30.43 \\ \hline
			\end{tabular}
		\end{center} 

		\subsection{Comparison with Original UDA Paper Results}
		Table \ref{tab:uda_comparison} compares our domain adaptation results with those reported in the original DACS paper \cite{tranheden2021dacs} and LoveDA benchmarks \cite{loveda2021}. While DACS achieved 39.10\% mIoU on LoveDA-rural in the original implementation, our PIDNet-S adaptation reached only 30.63\%. Similarly, adversarial training underperformed compared to Tsai et al. \cite{tsai2018learning} (30.59\% vs. 35.20\%). Three key factors explain these differences:
		
		\begin{itemize}
			\item \textbf{Model Architecture}: The original DACS paper used DeepLabV2 with ResNet-101, which has significantly higher capacity (44.5M params) compared to our real-time PIDNet-S (7.72M params). This architectural difference directly impacts feature representation power.
			\item \textbf{Training Constraints}: Our experiments used a fixed 20-epoch training schedule to maintain real-time deployment capabilities, whereas the original works employed longer training (50+ epochs) with extensive hyperparameter tuning.
			\item \textbf{Latency-Accuracy Trade-off}: PIDNet-S prioritizes inference speed (288ms) over pure accuracy, while DeepLabV2-based implementations ignore latency constraints (typically >1,000ms).
		\end{itemize}
		
		\justifying
		These results highlight the inherent challenge of balancing domain adaptation performance with real-time requirements—a critical consideration for edge deployment scenarios.
		
		\begin{center}
			% \centering
			% \caption{Comparison with Original UDA Paper Results (LoveDA-Rural)}
			\label{tab:uda_comparison}
			\resizebox{0.5\textwidth}{!}{%
			\begin{tabular}{|l|l|c|c|}
				\hline
				Method & Model & mIoU (\%) & Latency (ms) \\ \hline
				DACS (Original) \cite{tranheden2021dacs} & DeepLabV2 + ResNet-101 & 39.10 & 1,200 \\ \hline
				DACS (Ours) & PIDNet-S & 30.63 & 288 \\ \hline
				Adv. Training (Original) \cite{tsai2018learning} & DeepLabV2 + VGG16 & 35.20 & 850 \\ \hline
				Adv. Training (Ours) & PIDNet-S & 30.59 & 288 \\ \hline
			\end{tabular}%
		}
		\end{center} 

		\section{Conclusion}
		\justifying


		This study highlights the effectiveness of PIDNet for real-time semantic segmentation, achieving a strong performance on the LoveDA-urban dataset (288ms latency, 48.67\% mIoU). However, the impact of domain shift, as observed on LoveDA-rural (23.98\% mIoU), underscores the challenges of applying segmentation models across divergent environments. Among the evaluated mitigation strategies, photometric data augmentation (A2) showed the most consistent improvement (+6.82\% mIoU), while adversarial training and DACS offered moderate yet comparable gains (+6.61\% and +6.65\% mIoU, respectively). Despite these advancements, the trade-off between accuracy and computational efficiency remains a key limitation, particularly in edge computing scenarios. 
		\\
		Our experiments demonstrate that while PIDNet excels in maintaining real-time capabilities, alternative architectures like BiSeNetV1 can surpass it in domain-adaptive accuracy, at a slight cost to speed. These findings suggest that a hybrid approach, integrating the efficiency of PIDNet with enhanced feature representation techniques from other models, could be a promising direction.
		Future research could focus on:

		\begin{enumerate}
			\item \textbf {Hybrid Adaptation Strategies}: Combining DACS with adversarial training or exploring ensemble techniques to synergize their strengths.
			\item \textbf {Optimized Style Transfer Pipelines}: Refining preprocessing steps to reduce the spectral and textural differences between domains.
			\item \textbf {Domain-Specific Fine-Tuning}: Introducing lightweight domain-specific adaptations during deployment to address localized variations.
			\item \textbf {Longer Training Regimes}: Exploring optimized training schedules that balance performance improvements with real-time constraints.
		\end{enumerate}
		
		Ultimately, this work provides a foundation for advancing domain-adaptive real-time semantic segmentation, offering practical insights for deploying robust models in dynamic, real-world environments where domain variability is unavoidable.
		

		
	\end{multicols}
	
	\bibliographystyle{ieeetran}
	\bibliography{references}
	
\end{document}