take the following latex report for aml project and complete it based on our work

this is the original report:
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{hyperref}
\usepackage{booktabs}

\title{Real-time Domain Adaptation in Semantic Segmentation}
\author{Attrovio Mario, Ghisolfo Giorgia, Russo Michele}

\begin{document}
	\maketitle
	
	\begin{multicols}{2}
		
		\section{Abstract}
		Semantic segmentation is a critical task in computer vision, enabling pixel-wise classification of images. However, the performance of segmentation models often degrades when applied to data from different domains, a challenge known as domain shift. This report explores real-time semantic segmentation in the context of domain adaptation using PIDNet as the backbone. We investigate the performance drop caused by domain shift between urban and rural datasets and evaluate mitigation strategies, including data augmentation and advanced domain adaptation techniques like adversarial training and image-to-image translation (DACS). Experimental results on the LoveDA dataset demonstrate that these methods significantly reduce the impact of domain shift while maintaining real-time inference capabilities, achieving a balanced trade-off between accuracy and computational efficiency. The code can be found on our project website: \url{https://github.com/MichelePoli/AMLProject}.
		
		\section{Introduction}
		Semantic segmentation is a foundational task in computer vision, where each pixel in an image is assigned a label corresponding to a predefined class. It plays a vital role in applications such as autonomous driving, medical imaging, and remote sensing. Recent advancements in deep learning have yielded high-performing models, but these often struggle with domain shift—a phenomenon where a model trained on a source domain (e.g., urban images) performs poorly on a target domain (e.g., rural images). Addressing this challenge is crucial for real-world deployments where annotated data for all target domains is scarce or unavailable.
		
		Domain adaptation aims to bridge this performance gap by aligning the source and target domains without requiring extensive labeled data from the target domain. While several methods exist, real-time semantic segmentation introduces additional constraints, such as maintaining high inference speed and low computational cost. PIDNet \cite{pidnet2023}, a real-time segmentation network inspired by Proportional-Integral-Derivative (PID) controllers, serves as the backbone for our study due to its efficiency and accuracy in real-time tasks.
		
		This report focuses on evaluating and improving the performance of PIDNet for domain-adaptive semantic segmentation using the LoveDA dataset \cite{loveda2021}. We first quantify the performance degradation caused by domain shift. Next, we implement data augmentation techniques and two domain adaptation approaches—adversarial training and image-to-image translation (DACS)—to mitigate this issue. Our findings highlight the potential of these methods to enhance generalization while preserving the real-time capabilities of the model.
		
		\section{Related Work}
		Semantic segmentation has evolved significantly with deep learning. Fully Convolutional Networks (FCNs) \cite{long2015fcn} pioneered end-to-end segmentation by replacing fully connected layers with convolutional ones, enabling the network to process images of arbitrary sizes and produce pixel-wise predictions. DeepLabV2 \cite{chen2018deeplab} introduced atrous convolutions, which expand the receptive field without increasing the number of parameters, allowing for multi-scale context aggregation. Real-time networks like BiSeNet \cite{yu2018bisenet} optimized speed-accuracy trade-offs using lightweight backbones and parallel structures, while PIDNet \cite{pidnet2023} further improved efficiency by mimicking PID controllers to balance high-, mid-, and low-level features.
		
		Domain adaptation techniques address performance degradation across domains. Adversarial methods \cite{tsai2018learning} train a discriminator to distinguish between source and target domain features, encouraging the feature extractor to produce domain-invariant representations. Image-to-image translation approaches like DACS \cite{tranheden2021dacs} blend domains through mixed sampling, generating pseudo-labeled target domain images to improve model generalization. The LoveDA dataset \cite{loveda2021} provided urban/rural splits to benchmark these methods in remote sensing, offering a challenging scenario for domain adaptation research.
		
		\section{Methods}
		\subsection{Baseline Training}
		We first trained a classic semantic segmentation network, DeepLabV2 \cite{chen2018deeplab}, on the LoveDA-urban dataset for 20 epochs using a ResNet-101 backbone \cite{he2016deep} pre-trained on ImageNet. Then, we trained PIDNet-S \cite{pidnet2023}, also pre-trained on ImageNet, on LoveDA-urban for 20 epochs to establish an upper bound for real-time performance (mIoU: 48.67\%). Domain shift was quantified by testing this PIDNet-S model on LoveDA-rural (mIoU: 23.98\%).
		
		\subsection{Data Augmentation}
		Two augmentation strategies were applied during training with a probability of 0.5:
		\begin{itemize}
			\item \textbf{A1}: Geometric transforms (horizontal/vertical flips, 30° rotation)
			\item \textbf{A2}: Photometric transforms (ColorJitter, GaussianBlur)
		\end{itemize}
		The best single augmentation (A2) improved rural mIoU to 30.80\%, while combining A1 and A2 yielded 29.35\%.
		
		\subsection{Domain Adaptation}
		\textbf{Adversarial Training:} A discriminator \cite{tsai2018learning} was trained against PIDNet's features using a binary cross-entropy loss with a $\lambda$ value of 0.0005. The learning rate for the discriminator was set to $5 \times 10^{-4}$. This approach achieved 30.59\% mIoU on the target domain. \\
		\textbf{DACS:} Domain Adaptation via Cross-domain Mixed Sampling \cite{tranheden2021dacs} was implemented to blend classes between the source and target domains. This method reached 30.63\% mIoU on the target domain.
		
		\subsection{Extensions}
		\textbf{Style Transfer Preprocessing:} We applied style transfer using a pre-trained model to preprocess the source domain images, attempting to match the target domain's appearance. This resulted in an mIoU of 25.46\% on the target domain. \\
		\textbf{Alternative Models:} We explored two alternative real-time segmentation models:
		\begin{itemize}
			\item \textbf{BiSeNetV1} \cite{yu2018bisenet}: Achieved 32.21\% mIoU on the target domain.
			\item \textbf{LinkNet} \cite{chaurasia2017linknet}: Achieved 30.43\% mIoU on the target domain.
		\end{itemize}
		
		\section{Experimental Results}

	\centering
	\caption{Performance on LoveDA-Urban (Source Domain)}
	\label{tab:urban}
	\footnotesize % Adjust size if necessary
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		Model & mIoU (\%) & Latency (ms) & FLOPs & Params \\ \hline
		DeepLabV2 & 34.40 & - & - & - \\ \hline
		PIDNet-S & 48.67 & 288.70 & 6.35G & 7.72M \\ \hline
	\end{tabular}

	\centering
	\caption{Domain Shift: Urban $\rightarrow$ Rural}
	\label{tab:shift}
	\footnotesize % Adjust size if necessary
	\begin{tabular}{|l|c|c|c|c|c|c|c|}
		\hline
		Model & Road & Building & Water & Barren & Forest & Agric. & mIoU \\ \hline
		PIDNet & 16.34 & 23.99 & 35.46 & 3.12 & 8.83 & 31.82 & 23.98 \\ \hline
		+ A1 & 29.33 & 40.77 & 36.71 & 9.17 & 9.53 & 33.13 & 29.91 \\ \hline
		+ A2 & 31.41 & 38.37 & 31.33 & 10.26 & 15.10 & 37.51 & 30.80 \\ \hline
		+ A1 + A2 & 27.92 & 32.97 & 33.98 & 10.50 & 10.69 & 37.45 & 29.35 \\ \hline
		+ Adv. & 0.36 & 13.41 & 32.72 & 8.28 & 49.73 & 12.26 & 30.59 \\ \hline
		+ DACS & 31.32 & 36.60 & 42.70 & 4.49 & 2.70 & 40.60 & 30.63 \\ \hline
	\end{tabular}

	\centering
	\caption{Extensions on LoveDA-Rural (Target Domain)}
	\label{tab:extensions}
	\footnotesize % Adjust size if necessary
	\begin{tabular}{|l|c|}
		\hline
		Model & mIoU (\%) \\ \hline
		PIDNet + Style Transfer & 25.46 \\ \hline
		BiSeNetV1 & 32.21 \\ \hline
		LinkNet & 30.43 \\ \hline
	\end{tabular}
		\section{Conclusion}
		This work demonstrates PIDNet's effectiveness for real-time semantic segmentation on the LoveDA-urban dataset (288ms latency, 48.67\% mIoU) and quantifies the domain shift effect when applied to LoveDA-rural (23.98\% mIoU). Data augmentation, particularly photometric transforms (A2), provided the most consistent improvement (+6.82\% mIoU), while adversarial training and DACS showed moderate gains (+6.61\% and +6.65\% mIoU, respectively). Extensions with alternative models highlighted PIDNet's superior speed-accuracy balance, although BiSeNetV1 performed slightly better on the target domain. Future work could explore hybrid adaptation strategies, optimized style transfer pipelines, and the combination of DACS and adversarial training for further performance enhancements.
		
	\end{multicols}
	
	\bibliographystyle{ieeetran}
	\bibliography{references}
\end{document}
this is our TODO:



Real-time Domain Adaptation in 
Semantic Segmentation
TA: Claudia Cuttano (claudia.cuttano@polito.it)


OVERVIEW
The main objective of this project is to become familiar with the task of Domain Adaptation applied to the Real-time Semantic Segmentation networks. The student should understand the general approaches to perform Domain Adaptation in Semantic Segmentation and the main reason to apply them to  real-time networks. Before starting, the student should read [1] [2] [3] [4] [5] to get familiar with the tasks. As the next step, the student should train the real-time segmentation network [4] on the source dataset [6]-urban to define the upper bound. Then, he/she should train the network [4] on the source dataset [6]-urban and evaluate the drop of the performance when directly testing the trained model on the target images [6]-rural.  For the last part of the project, the student should implement two domain adaptation techniques [7][8] to mitigate the performance drop.

USEFUL LINKS
Dataset: https://zenodo.org/records/5706578
Note: download Train.zip to extract the training images for the Urban/Rural splits. Download Val.zip to extract the validation images for the Rural split.  
Model: https://github.com/XuJiacong/PIDNet




1st STEP) RELATED WORKS
Reading paper to get familiar with the task 
Before starting it is mandatory to take time to familiarize yourself with the tasks of Semantic Segmentation, Domain Adaptation and Real-time Semantic Segmentation. It is compulsory to understand what are the main problems and the main solutions to tackle them in literature. More in detail, read: 
[1] to understand Semantic Segmentation;
[2][3][4] to understand classic and real-time solutions for Semantic Segmentation;
[5] to get familiar with the several solutions to perform unsupervised domain adaptation in Semantic Segmentation;
[5] [6] to get familiar with the datasets that will be used in this project; 

2nd STEP) TESTING SEMANTIC SEGMENTATION NETWORKS
Classic semantic segmentation network.
For this step, you have to train a classic segmentation network (DeepLabV2 [2]) on the LoveDA-urban dataset. 
Dataset: LoveDA-urban [6]
Training epochs: 20
Backbone: R101 (pre-trained on ImageNet) [2]
Metrics: Mean Intersection over Union (mIoU) [read this to understand the metrics], latency, FLOPs, number of parameters.

Table 1) LoveDA
mIoU (%)
Latency
FLOPs
Params
DeepLabV2 - 20 epochs










Real-time semantic segmentation network.
For this step, you have to train a real-time segmentation network (PIDNet [4]) on the LoveDA-urban dataset. 
Dataset:  LoveDA-urban dataset [6]
Training epochs: 20
Backbone: PIDNet-S (pre-trained on ImageNet) [3]
Metrics: mIoU, latency, FLOPs, number of parameters.

Table 2) Real-time LoveDA
mIoU (%)
Latency
FLOPs
Params
PIDNet - 20 epochs










3rd STEP) DOMAIN SHIFT
From now on, we will employ PIDNet as our segmentation to ease the resource requirements of the next experiments. Consider as upper bound the results obtained in Table 2, i.e. the segmentation networks trained on the labeled source images (LoveDA-urban).
Evaluating the domain shift problem in Semantic Segmentation 
In semantic segmentation collecting manually annotated images is expensive. To this end, in this step we employ the images from LoveDA-urban [6] (source domain) to train our real-time segmentation network, which is then evaluated on the target images from LoveDA-rural [6] (target domain).
Training Set:  LoveDA-urban [6] 
Validation Set: LoveDA-rural [6]
Training epochs: 20
Metrics: mIoU

Table 3) Domain Shift
Urban → Rural
mIoU (%)
road
building
water
…
…
PIDNet - 20 epochs














How the performance change with respect to Table 2? Why?

Data augmentations to reduce the domain shift 
A naive solution to improve the generalization capability of the segmentation network consists in the usage of data augmentations during training. Through them, we i) virtually expand the dataset size and ii) modify the visual appearance of source images in order to make them more similar to the target ones. 
Specifically, we repeat the previous experiment, introducing data augmentations at training time (e.g. horizontal flip, Gaussian Blur, Multiply, ecc.). The decision of what kind of algorithm is left to the student. Set the probability to perform augmentation to 0.5.

Table 4) Augmentations
Urban → Rural
mIoU (%)
road
building
water
…
PIDNet - 20 epochs - Aug. 1










PIDNet - 20 epochs - Aug. 2










PIDNet - 20 epochs - Aug. 1 + Aug. 2










…












4th STEP) DOMAIN ADAPTATION
To effectively tackle the problem of domain shift, various domain adaptation techniques has been proposed. Domain adaptation solutions are mainly divided into two approaches:
Adversarial approaches. These methods involve a game between two parts of a model. One part, the feature extractor, tries to learn features that are indistinguishable between the source and target domains. The other part, the discriminator, tries to tell whether those features came from the source or target domain. This push-and-pull leads to features that are both discriminative (useful for the task) and domain-invariant (work well on both domains).
Image-to-image translation approaches. The focus lies in translating images from the source domain to resemble the style of the target domain. The idea is that if we can make images from the different domains look similar, the model's performance will transfer more easily.

4a) Adversarial approach

You can assume: 
Source Labelled Dataset: LoveDA-urban [6]
Target Unlabelled Dataset: LoveDA-rural [6]
Implement discriminator function, like in [7]
Take the best setting of step 3b (data augmentation) and perform training. 

Table 5) Adversarial
Urban → Rural
mIoU (%)
road
building
water
…
…
PIDNet - 20 epochs















4b) Image-to-image approach

You can assume: 
Source Labelled Dataset: LoveDA-urban [6]
Target Unlabelled Dataset: LoveDA-rural [6]
Implement image-to-image adaptations: DACS [8] 
Take the best setting of step 3b (data augmentation) and perform training. 

Table 5) Image-to-image
Urban → Rural
mIoU (%)
road
building
water
…
…
PIDNet - DACS - 20 epochs















5th STEP) EXTENSION
The final step of the project involves exploring additional techniques or modifications to further improve the performance of the domain adaptation task. Here some examples:
Apply Style Transfer Preprocessing: Preprocess source domain images with a style-transfer model to match the target domain's appearance.
Explore alternative real-time networks (e.g.  STDC PDF, PEM PDF, …).
Explore Alternative Segmentation Losses: Investigate how using different losses impacts performance and domain adaptation (simpler alternative).
Hyperparameter Tuning: Explore different learning rates, batch sizes, and data augmentation probabilities to optimize performance (simpler alternative).
… use your imagination!




REFERENCES 
[1] “A Brief Survey on Semantic Segmentation with Deep Learning”, Shijie Hao, Yuan Zhou, Yanrong Guo, PDF 
[2] “DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFS”, Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille PDF
[3] “BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation”, Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang, PDF
[4] PIDNet: “A Real-time Semantic Segmentation Network Inspired by PID Controllers”,  PDF
[5] “A Review of Single-Source Deep Unsupervised Visual Domain Adaptation”, Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen Wu, Ravi Krishna, Joseph E. Gonzalez, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia, Kurt Keutzer, PDF
[6] “LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation”, PDF
[7] “Learning to Adapt Structured Output Space for Semantic Segmentation”,
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker, PDF
[8] “DACS: Domain Adaptation via Cross-domain Mixed Sampling”, Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, Lennart Svensson, PDF


THIS is our work:

2a 

Validation Loss: 1.2985
IoU: [0.46287403 0.3011     0.17018034 0.49119395 0.11249792 0.2613182
 0.6085336 ]
mean IoU: 0.3439568877220154


2b

Validation mIoU: 0.4517
Best Validation mIoU: 0.4867


Evaluating Model Performance:
Latency: 288.70 ms
FLOPs: 6.346G
Params: 7.718M

Class-wise IoU:
Class 1 IoU: 0.5047
Class 2 IoU: 0.5444
Class 3 IoU: 0.4687
Class 4 IoU: 0.6341
Class 5 IoU: 0.2235
Class 6 IoU: 0.3183
Class 7 IoU: 0.4681

3a
 best in 20 epoch
Epoch [10/20] - Validation mIoU: 0.2398
Epoch [10/20] - Per-Class IoU: {'Background': 0.4716426432132721, 'Building': 0.23997391760349274, 'Road': 0.16344159841537476, 'Water': 0.35457953810691833, 'Barren': 0.031160786747932434, 'Forest': 0.08831749856472015, 'Agricultural': 0.3181770443916321}
Epoch [11/20] - Average Training Loss: 0.3635

3b

solo A2 
Epoch [18/20] - Validation mIoU: 0.3080
Epoch [18/20] - Per-Class IoU: {'Background': 0.515929102897644, 'Building': 0.3837207853794098, 'Road': 0.31411319971084595, 'Water': 0.3133336007595062, 'Barren': 0.10261649638414383, 'Forest': 0.15097035467624664, 'Agricultural': 0.3751249313354492}
Epoch 19/20: 100%|██████████| 109/109 [02:18<00:00,  1.28s/batch, loss=0.4344]
Epoch [19/20] Loss: 0.3132
SOLO a1 
Epoch [16/20] - Validation mIoU: 0.2991
Epoch [16/20] - Per-Class IoU: {'Background': 0.5073935985565186, 'Building': 0.40765082836151123, 'Road': 0.29326069355010986, 'Water': 0.36705729365348816, 'Barren': 0.09173088520765305, 'Forest': 0.09528797119855881, 'Agricultural': 0.33129584789276123}
Epoch 17/20: 100%|██████████| 109/109 [02:19<00:00,  1.28s/batch, loss=0.3289]
Epoch [17/20] Loss: 0.3062
a1+A2
Epoch [11/20] - Validation mIoU: 0.2935
Epoch [11/20] - Per-Class IoU: {'Background': 0.5192663073539734, 'Building': 0.32970282435417175, 'Road': 0.27916231751441956, 'Water': 0.33984655141830444, 'Barren': 0.10504362732172012, 'Forest': 0.10686410218477249, 'Agricultural': 0.37451180815696716}
Epoch 12/20: 100%|██████████| 145/145 [03:00<00:00,  1.25s/batch, loss=0.3271]
Epoch [12/20] Loss: 0.3003
Augmentations
 # Applica augmentazione se abilitata
        if self.aug1 and aug_1:
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            transform_aug1 = transforms.Compose([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.5),
                transforms.RandomRotation(30)
            ])
            image = Image.fromarray(image_np)
            image = transform_aug1(image)
            image_np = np.array(image)

            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            label = Image.fromarray(label_np)
            label = transform_aug1(label)
            label_np = np.array(label)

        if self.aug2 and aug_2:
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            strong_parameters = {}
            strong_parameters["ColorJitter"] = random.uniform(0, 1)
            strong_parameters["GaussianBlur"] = random.uniform(0, 1)


            image = Image.fromarray(image_np)
            image, _ = strongTransform(strong_parameters, image, None)
            image_np = np.array(image)

            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            label = Image.fromarray(label_np)
            # label = transform_aug2(label)
            label_np = np.array(label)

        # Trasformazioni opzionali per immagine e maschera
        if self.transform_image:
            transform_image_alb = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ])
            image = Image.fromarray(image_np)
            image = transform_image_alb(image)

        if self.transform_label:
            transform_label_alb = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor()
            ])
            label = Image.fromarray(label_np)
            label = transform_label_alb(label).squeeze(0)
            label = color_to_one_hot(label, LABEL_MAP)

        return image,label
4a

adversarial

Epoch [9/20] - Target mIoU: 0.3059
Epoch [9/20] - Per-Class IoU: {'Background': 0.41577833890914917, 'Building': 0.13407956063747406, 'Road': 0.003580126678571105, 'Water': 0.3271559476852417, 'Barren': 0.08280465006828308, 'Forest': 0.497349351644516, 'Agricultural': 0.12262508273124695}
learning_rate_D = 5e-4


bce_loss = torch.nn.BCEWithLogitsLoss()

model_D1 = FCDiscriminator(num_classes=7)

lambda_adv_1 = 0.0005

4b

Epoch 5/20: 100%|██████████| 62/62 [02:38<00:00,  2.55s/batch]
Epoch [5/20] - Loss: 0.7993
Validating Epoch 5/20: 100%|██████████| 62/62 [01:16<00:00,  1.23s/batch]
Epoch [5/20] - Validation mIoU: 0.3063
Epoch [5/20] - Per-Class IoU: {'Background': 0.5598487257957458, 'Building': 0.36597740650177, 'Road': 0.3131501376628876, 'Water': 0.4269826412200928, 'Barren': 0.0448836050927639, 'Forest': 0.026997920125722885, 'Agricultural': 0.4059675335884094}
def classmix(source_images, source_labels, target_images, target_labels, num_classes=7):
    """
    Perform classmix image augmentation, mixing source domain image and target domain image pixels based on class masks.
    """
    batch_size, _, height, width = source_images.size()
    #print("sourceLabels",source_labels)

    # Ensure source_labels is an integer type
    source_labels = source_labels.long()

    # Generate a random binary mask for each class
    class_masks = torch.zeros((batch_size, height, width), device=source_images.device, dtype=torch.bool)
    #print(class_masks)
    #print(class_masks.shape) #torch.Size([2, 512, 512])
    #print(source_labels.shape) #torch.Size([2, 7, 512, 512])
    #source_labels = torch.argmax(source_labels, dim=1)  # Convert to class indices (batch_size, height, width)

    for b in range(batch_size):
        best_proportion_source = 0.0  # To store the best proportion found
        best_class_mask = torch.zeros((height, width), device=source_images.device, dtype=torch.bool)  # Best mask found
        count = 0  # Initialize the count for while loop iterations
        proportion_source=0.0


        # Keep regenerating the mask until the proportion is within the desired range or count reaches 10
        while (proportion_source < 0.1 or proportion_source > 0.9) and count < 10:
            # Randomly select half of the classes for each image in the batch
            selected_classes = torch.randperm(num_classes)[:num_classes // 2]
            #print("selected_classes: ", selected_classes)

            # Reset the mask for this batch
            class_masks[b] = torch.zeros((height, width), device=source_images.device, dtype=torch.bool)

            for cls in selected_classes:
                class_masks[b] = class_masks[b] | (source_labels[b] == cls)

            # Calculate the proportion of source pixels in the mask
            proportion_source = class_masks[b].float().mean()
            #print("proportion_source:", proportion_source)

            # If this proportion is the best so far, store the mask and proportion
            if proportion_source > best_proportion_source:
                best_proportion_source = proportion_source
                best_class_mask = class_masks[b].clone()  # Store the best mask

            # Increment the count
            count += 1

            # If the count reaches 10, exit the loop
            if count >= 10:
                #print(f"Maximum iterations reached for batch {b}. Proportion did not satisfy the condition.")
                break

        # After the loop, use the best mask found
        #print(f"Best proportion_source for batch {b}: {best_proportion_source}")
        class_mask_rgb = best_class_mask.unsqueeze(0).repeat(3, 1, 1).float() * 255  # Convert to RGB
        #show_image(class_mask_rgb) #print the mask

        # Update the class mask for this batch
        class_masks[b] = best_class_mask

    # Expand the mask to match the channel dimension for images
    class_masks_expanded = class_masks.unsqueeze(1).expand(-1, source_images.size(1), -1, -1)


    # Mix images using the masks
    mixed_images = torch.where(class_masks_expanded, source_images, target_images)

    # Mix labels using the same masks
    mixed_labels = torch.where(class_masks, source_labels, target_labels)


    return mixed_images, mixed_labels

5

Style Transfer Preprocessing
!git clone https://github.com/nazianafis/Neural-Style-Transfer.git
Epoch [4/20] - Validation mIoU: 0.2546
Epoch [4/20] - Per-Class IoU: {'Background': 0.4453784227371216, 'Building': 0.30618488788604736, 'Road': 0.2511318325996399, 'Water': 0.4815290570259094, 'Barren': 0.07658208906650543, 'Forest': 0.11567701399326324, 'Agricultural': 0.10596276819705963}
Epoch 5/20: 100%|██████████| 143/143 [01:37<00:00,  1.46batch/s, loss=0.3308]
Epoch [5/20] Loss: 0.3208
trained only on training transferred style from target to source, augmented with A1 and A2



bisenetv1
Epoch [20/20] - Per-Class IoU: {'Background': 0.3780554234981537, 'Building': 0.14438600838184357, 'Road': 0.20330047607421875, 'Water': 0.4616515040397644, 'Barren': 0.11092692613601685, 'Forest': 0.4166441559791565, 'Agricultural': 0.12552829086780548}
Best Validation mIoU: 0.3221


linknet

Epoch [8/20] - Target mIoU: 0.3043
Epoch [8/20] - Per-Class IoU: {'Background': 0.37441688776016235, 'Building': 0.29231518507003784, 'Road': 0.21871599555015564, 'Water': 0.4186348617076874, 'Barren': 0.10939700901508331, 'Forest': 0.39345771074295044, 'Agricultural': 0.32292360067367554}


papers important concepts










MAKE tables in latex with metrics and results for each step

add method part each model, why, losses, learning rates, approach, domain adaptation, limitations etc 

i want a 4 page report

DO NOT INSERT code in the latex file, just complete the report based on the information provided in the prompt and the work we have done.
CITE papers and insert key cited papers concepts in the report.






