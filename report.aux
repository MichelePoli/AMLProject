\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{tranheden2021dacs}
\citation{loveda2021}
\citation{loveda2021}
\citation{pidnet2023}
\citation{loveda2021}
\citation{tranheden2021dacs}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}{section.2}\protected@file@percent }
\newlabel{fig:dataset_rural_image1}{{2}{1}{Introduction}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Satellite imagery: (Top) Rural images, (Bottom) urban images }}{1}{figure.1}\protected@file@percent }
\newlabel{fig:dataset_urban_image1}{{1}{1}{Satellite imagery: (Top) Rural images, (Bottom) urban images}{figure.1}{}}
\citation{long2015fcn}
\citation{chen2018deeplab}
\citation{yu2018bisenet}
\citation{pidnet2023}
\citation{tsai2018learning}
\citation{tranheden2021dacs}
\citation{chen2018deeplab}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Algorithmic overview. Given images with the size H by W in source and target domains, we pass them through the segmentation network to obtain output predictions. For source predictions with C categories, a segmentation loss is computed based on the source ground truth. To make target predictions closer to the source ones, we utilize a discriminator to distinguish whether the input is from the source or target domain. Then an adversarial loss is calculated on the target prediction and is back-propagated to the segmentation network. We call this process as one adaptation module, and we illustrate our proposed multi-level adversarial learning by adopting two adaptation modules at two different levels here.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:adversarial_model}{{2}{2}{Algorithmic overview. Given images with the size H by W in source and target domains, we pass them through the segmentation network to obtain output predictions. For source predictions with C categories, a segmentation loss is computed based on the source ground truth. To make target predictions closer to the source ones, we utilize a discriminator to distinguish whether the input is from the source or target domain. Then an adversarial loss is calculated on the target prediction and is back-propagated to the segmentation network. We call this process as one adaptation module, and we illustrate our proposed multi-level adversarial learning by adopting two adaptation modules at two different levels here}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related work}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline model development}{2}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}2a Classic segmentation architecture }{2}{subsubsection.4.1.1}\protected@file@percent }
\citation{pidnet2023}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An overview of the basic architecture of our proposed Proportional-Integral-Derivative Network (PIDNet)}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:PIDNET_image}{{3}{3}{An overview of the basic architecture of our proposed Proportional-Integral-Derivative Network (PIDNet)}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}2b Efficient segmentation framework}{3}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}PIDNet architecture details}{3}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}3a Domain adaptation }{3}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:domain_shift}{{4.2}{3}{3a Domain adaptation}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of urban image (left) and rural image (right)}}{3}{figure.4}\protected@file@percent }
\newlabel{fig:dataset_mix}{{4}{3}{Example of urban image (left) and rural image (right)}{figure.4}{}}
\citation{tsai2018learning}
\citation{tranheden2021dacs}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Domain adaptation challenges}{4}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The left image is modified by augmentation A1, while the right image is modified by augmentation A2.}}{4}{figure.5}\protected@file@percent }
\newlabel{fig:data_aug}{{5}{4}{The left image is modified by augmentation A1, while the right image is modified by augmentation A2}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}3b Data augmentation}{4}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Unsupervised domain adaptation}{4}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Phase 1: Source domain training}{4}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Phase 2: Adversarial alignment}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Phase 3: Discriminator training}{4}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (top left) urban sample, (top right) rural sample, (bottom) hybrid sample}}{4}{figure.6}\protected@file@percent }
\newlabel{fig:DACS_image_result}{{6}{4}{(top left) urban sample, (top right) rural sample, (bottom) hybrid sample}{figure.6}{}}
\citation{yu2018bisenet}
\citation{chaurasia2017linknet}
\citation{fan2021rethinking}
\citation{loveda2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}4b DACS}{5}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Diagram showing DACS. The images X\textsubscript  {S} and X\textsubscript  {T} are mixed together, using Y\textsubscript  {S} for the labels of X\textsubscript  {S}, instead of a pre dicted semantic map to determine the binary mask. The segmenta tion network is then trained on both batches of augmented images and images from the source dataset.}}{5}{figure.7}\protected@file@percent }
\newlabel{fig:DACS_image}{{7}{5}{Diagram showing DACS. The images X\textsubscript {S} and X\textsubscript {T} are mixed together, using Y\textsubscript {S} for the labels of X\textsubscript {S}, instead of a pre dicted semantic map to determine the binary mask. The segmenta tion network is then trained on both batches of augmented images and images from the source dataset}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Extensions}{5}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Class distribution similarities between urban (top) and rural (bottom) domains in LoveDA: the background class dominates both domains.}}{5}{figure.8}\protected@file@percent }
\newlabel{fig:class_eug}{{8}{5}{Class distribution similarities between urban (top) and rural (bottom) domains in LoveDA: the background class dominates both domains}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental results}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}LoveDA dataset}{6}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Class distribution diversity between urban (top) and rural (bottom) domains in LoveDA.}}{6}{figure.9}\protected@file@percent }
\newlabel{fig:class_div}{{9}{6}{Class distribution diversity between urban (top) and rural (bottom) domains in LoveDA}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance on source domain}{6}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison of different hyperparameter tuning.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:urban}{{1}{6}{Performance comparison of different hyperparameter tuning}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Domain shift evaluation}{6}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces urban image }}{6}{figure.10}\protected@file@percent }
\newlabel{fig:transfer}{{10}{6}{urban image}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Extensions}{6}{subsection.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance comparison of different models on rural images.}}{7}{table.2}\protected@file@percent }
\newlabel{tab:extensions}{{2}{7}{Performance comparison of different models on rural images}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Hyperparameter tuning Rresults and analysis}{7}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance comparison of different models on urban images.}}{7}{table.3}\protected@file@percent }
\newlabel{tab:hyperparameters}{{3}{7}{Performance comparison of different models on urban images}{table.3}{}}
\newlabel{tab:shift}{{}{7}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Performance comparison of different version of aumented dataset.}}{7}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Loss function comparison}{7}{subsubsection.5.4.2}\protected@file@percent }
\citation{tranheden2021dacs}
\citation{loveda2021}
\citation{tsai2018learning}
\citation{tranheden2021dacs}
\citation{tsai2018learning}
\bibstyle{ieeetran}
\bibdata{references}
\bibcite{tranheden2021dacs}{1}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performance comparison of different loss functions.}}{8}{table.5}\protected@file@percent }
\newlabel{tab:loss_functions}{{5}{8}{Performance comparison of different loss functions}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Comparison with original UDA paper results}{8}{subsection.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance comparison with original UDA paper results.}}{8}{table.6}\protected@file@percent }
\newlabel{tab:uda_comparison}{{6}{8}{Performance comparison with original UDA paper results}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\bibcite{loveda2021}{2}
\bibcite{pidnet2023}{3}
\bibcite{long2015fcn}{4}
\bibcite{chen2018deeplab}{5}
\bibcite{yu2018bisenet}{6}
\bibcite{tsai2018learning}{7}
\bibcite{chaurasia2017linknet}{8}
\bibcite{fan2021rethinking}{9}
\gdef \@abspage@last{9}
