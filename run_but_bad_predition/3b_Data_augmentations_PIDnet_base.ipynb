{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEuvsfgGyTR1"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install thop\n",
        "!pip install yacs\n",
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5_pqmv3yTR2"
      },
      "outputs": [],
      "source": [
        "#import\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from thop import profile, clever_format\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import kornia\n",
        "import time\n",
        "import logging\n",
        "from torch.utils import data, model_zoo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ETFJcNTyTR3",
        "outputId": "46c9cb02-934b-4c4e-ce1d-51326163b408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 10:21:36--  https://zenodo.org/record/5706578/files/Train.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Train.zip [following]\n",
            "--2025-01-09 10:21:37--  https://zenodo.org/records/5706578/files/Train.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘Train.zip’\n",
            "\n",
            "Train.zip            59%[==========>         ]   2.21G  17.0MB/s    eta 94s    ^C\n",
            "--2025-01-09 10:23:53--  https://zenodo.org/record/5706578/files/Val.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.48.194, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5706578/files/Val.zip [following]\n",
            "--2025-01-09 10:23:53--  https://zenodo.org/records/5706578/files/Val.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘Val.zip’\n",
            "\n",
            "Val.zip              18%[==>                 ] 425.39M  11.8MB/s    eta 16m 55s^C\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/5706578/files/Train.zip\n",
        "!wget https://zenodo.org/record/5706578/files/Val.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eNVmWJxMLTH",
        "outputId": "5f992777-bf70-4c5d-d53f-7f5eb704fc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 10:27:38--  https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38061375 (36M) [application/octet-stream]\n",
            "Saving to: ‘PIDNet_S_ImageNet.pth.tar’\n",
            "\n",
            "PIDNet_S_ImageNet.p  21%[===>                ]   7.88M  2.33MB/s    eta 13s    "
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/records/14606189/files/PIDNet_S_ImageNet.pth.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-_nsbr6yTR3"
      },
      "outputs": [],
      "source": [
        "# Define file paths\n",
        "train_zip = \"Train.zip\"\n",
        "val_zip = \"Val.zip\"\n",
        "\n",
        "# Extract Train.zip\n",
        "with zipfile.ZipFile(train_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Train\")\n",
        "\n",
        "# Extract Val.zip\n",
        "with zipfile.ZipFile(val_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"Val\")\n",
        "\n",
        "# Verify the extracted folders\n",
        "print(\"Train Directory Contents:\", os.listdir(\"Train\"))\n",
        "print(\"Val Directory Contents:\", os.listdir(\"Val\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s2uiuRUyTR4"
      },
      "outputs": [],
      "source": [
        "# Load and visualize an image and its corresponding label\n",
        "image_path = \"Train/Train/Urban/images_png/1366.png\"\n",
        "label_path = \"Train/Train/Urban/masks_png/1366.png\"\n",
        "\n",
        "image = Image.open(image_path)\n",
        "label = Image.open(label_path)\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Label\")\n",
        "plt.imshow(label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOPr83U7yTR5"
      },
      "outputs": [],
      "source": [
        "#TASK 3.b\n",
        "\n",
        "COLOR_MAP = {\n",
        "    'Background': 0.00392157,\n",
        "    'Building': 0.00784314,\n",
        "    'Road': 0.01176471,\n",
        "    'Water': 0.01568628,\n",
        "    'Barren': 0.01960784,\n",
        "    'Forest': 0.02352941,\n",
        "    'Agricultural': 0.02745098\n",
        "}\n",
        "\n",
        "LABEL_MAP = OrderedDict(\n",
        "    Background=0,\n",
        "    Building=1,\n",
        "    Road=2,\n",
        "    Water=3,\n",
        "    Barren=4,\n",
        "    Forest=5,\n",
        "    Agricultural=6\n",
        ")\n",
        "\n",
        "# Tolerance-based color to label conversion\n",
        "def color_to_one_hot(mask, label_map, tolerance=0.001, num_classes=7):\n",
        "    mask = np.asarray(mask, dtype=np.float32)\n",
        "    if len(mask.shape) == 2:  # Ensure channel dimension exists\n",
        "        mask = np.expand_dims(mask, axis=0)  # Convert [H, W] to [1, H, W]\n",
        "\n",
        "    one_hot_mask = np.zeros((num_classes, mask.shape[1], mask.shape[2]), dtype=np.float32)\n",
        "\n",
        "    for class_name, class_index in label_map.items():\n",
        "        color_value = COLOR_MAP[class_name]\n",
        "        # Match pixels with the grayscale value within the tolerance\n",
        "        matches = np.abs(mask - color_value) < tolerance\n",
        "        one_hot_mask[class_index, np.squeeze(matches, axis=0)] = 1.0\n",
        "\n",
        "    return torch.from_numpy(one_hot_mask)\n",
        "\n",
        "\n",
        "# Define a function to denormalize the image\n",
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\" Denormalize the tensor back to the [0, 1] range for visualization. \"\"\"\n",
        "    for i in range(len(mean)):\n",
        "        tensor[i] = tensor[i] * std[i] + mean[i]\n",
        "    return tensor\n",
        "\n",
        "def show_image(image_tensor):\n",
        "    \"\"\" Display a transformed image using matplotlib. \"\"\"\n",
        "    image_tensor = denormalize(image_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Denormalize the image\n",
        "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert to HxWxC format\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "def show_mask(mask_tensor, num_classes=7):\n",
        "    \"\"\" Display the mask using matplotlib. \"\"\"\n",
        "    # Convert the one-hot mask to the label mask (indices of the highest value per pixel)\n",
        "    mask = mask_tensor.squeeze().cpu().numpy()  # Remove the batch dimension and convert to numpy\n",
        "    label_mask = np.argmax(mask, axis=0)  # Get the class index with the highest value for each pixel\n",
        "\n",
        "    plt.imshow(label_mask, cmap='tab20')  # Use a colormap suitable for categorical data\n",
        "    plt.colorbar()  # Optionally add a color bar\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def colorJitter(colorJitter, img_mean, data = None, target = None, s=0.25):\n",
        "    # s is the strength of colorjitter\n",
        "    #colorJitter\n",
        "    if not (data is None):\n",
        "        if data.shape[1]==3:\n",
        "            if colorJitter > 0.2:\n",
        "                img_mean, _ = torch.broadcast_tensors(img_mean.unsqueeze(0).unsqueeze(2).unsqueeze(3), data)\n",
        "                seq = nn.Sequential(kornia.augmentation.ColorJitter(brightness=s,contrast=s,saturation=s,hue=s))\n",
        "                data = (data+img_mean)/255\n",
        "                data = seq(data)\n",
        "                data = (data*255-img_mean).float()\n",
        "    return data, target\n",
        "\n",
        "def gaussian_blur(blur, data = None, target = None):\n",
        "    if not (data is None):\n",
        "        if data.shape[1]==3:\n",
        "            if blur > 0.5:\n",
        "                sigma = np.random.uniform(0.15,1.15)\n",
        "                kernel_size_y = int(np.floor(np.ceil(0.1 * data.shape[2]) - 0.5 + np.ceil(0.1 * data.shape[2]) % 2))\n",
        "                kernel_size_x = int(np.floor(np.ceil(0.1 * data.shape[3]) - 0.5 + np.ceil(0.1 * data.shape[3]) % 2))\n",
        "                kernel_size = (kernel_size_y, kernel_size_x)\n",
        "                seq = nn.Sequential(kornia.filters.GaussianBlur2d(kernel_size=kernel_size, sigma=(sigma, sigma)))\n",
        "                data = seq(data)\n",
        "    return data, target\n",
        "\n",
        "def strongTransform(parameters, data=None, target=None):\n",
        "    assert ((data is not None) or (target is not None))\n",
        "    # data, target = transformsgpu.oneMix(mask = parameters[\"Mix\"], data = data, target = target)\n",
        "    data, target = colorJitter(colorJitter = parameters[\"ColorJitter\"], img_mean = torch.from_numpy(IMG_MEAN.copy()).cuda(), data = data, target = target)\n",
        "    data, target = gaussian_blur(blur = parameters[\"GaussianBlur\"], data = data, target = target)\n",
        "    # data, target = transformsgpu.flip(flip = parameters[\"flip\"], data = data, target = target)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform_image=None, transform_label=None, subdir=\"Rural\", aug1=False, aug2=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_label = transform_label\n",
        "        self.aug1 = aug1\n",
        "        self.aug2 = aug2\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        image_dir = os.path.join(root_dir, split, subdir, 'images_png')\n",
        "        label_dir = os.path.join(root_dir, split, subdir, 'masks_png')\n",
        "\n",
        "        for f in os.listdir(image_dir):\n",
        "            if f.endswith('.png'):\n",
        "                self.image_paths.append(os.path.join(image_dir, f))\n",
        "                self.label_paths.append(os.path.join(label_dir, f))\n",
        "\n",
        "    def __len__(self):\n",
        "        original_length = len(self.image_paths)\n",
        "        if self.aug1 and self.aug2:\n",
        "            return original_length * 2\n",
        "        elif self.aug1 or self.aug2:\n",
        "            return int(original_length * 1.5)\n",
        "        else:\n",
        "            return original_length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_length = len(self.image_paths)\n",
        "\n",
        "        # Calcola l'indice originale nel dataset\n",
        "        idx = idx % original_length\n",
        "\n",
        "        aug_1 = False\n",
        "        aug_2 = False\n",
        "        if(idx < original_length):\n",
        "            aug_1 = False\n",
        "            aug_2 = False\n",
        "\n",
        "        elif(idx >= original_length):\n",
        "            if(random.random() < 0.5):\n",
        "                if(self.aug1):\n",
        "                    aug_1 = True\n",
        "                    aug_2 = False\n",
        "                elif(self.aug2):\n",
        "                    aug_1 = False\n",
        "                    aug_2 = True\n",
        "\n",
        "            else:\n",
        "                if (self.aug2):\n",
        "                    aug_1 = False\n",
        "                    aug_2 = True\n",
        "                elif(self.aug1):\n",
        "                    aug_1 = True\n",
        "                    aug_2 = False\n",
        "\n",
        "\n",
        "        # Carica l'immagine e la maschera\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = Image.open(self.label_paths[idx]).convert(\"L\")\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        label_np = np.array(label)\n",
        "\n",
        "        # Seed per garantire coerenza tra immagine e maschera\n",
        "        seed = np.random.randint(2147483647)\n",
        "\n",
        "        # Applica augmentazione se abilitata\n",
        "        if self.aug1 and aug_1:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            transform_aug1 = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(30)\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_aug1(image)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_aug1(label)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        if self.aug2 and aug_2:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            # transform_aug2 = transforms.Compose([\n",
        "            #     #sigma=(0.1, 5) indica un intervallo di valori tra 0.1 e 5, quindi la sfocatura potrebbe variare in base a come viene applicata.\n",
        "            #     transforms.GaussianBlur(kernel_size=(5,9), sigma=( 0.1, 5)) #valori di dimensioni (5, 9), che significa che il filtro si estende su un'area di 5x9 pixel.\n",
        "            # ])\n",
        "            strong_parameters = {}\n",
        "            strong_parameters[\"ColorJitter\"] = random.uniform(0, 1)\n",
        "            strong_parameters[\"GaussianBlur\"] = random.uniform(0, 1)\n",
        "\n",
        "\n",
        "            image = Image.fromarray(image_np)\n",
        "            image, _ = strongTransform(strong_parameters, image, None)\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            label = Image.fromarray(label_np)\n",
        "            # label = transform_aug2(label)\n",
        "            label_np = np.array(label)\n",
        "\n",
        "        # Trasformazioni opzionali per immagine e maschera\n",
        "        if self.transform_image:\n",
        "            transform_image_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "            image = Image.fromarray(image_np)\n",
        "            image = transform_image_alb(image)\n",
        "\n",
        "        if self.transform_label:\n",
        "            transform_label_alb = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            label = Image.fromarray(label_np)\n",
        "            label = transform_label_alb(label).squeeze(0)\n",
        "            label = color_to_one_hot(label, LABEL_MAP)\n",
        "\n",
        "        return image,label\n",
        "\n",
        "# Define transformations without augmentations\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "transform_label = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Set augmentation flags\n",
        "AUG1 = False\n",
        "AUG2 = False\n",
        "\n",
        "BATCH_SIZE = 16 #or 32, or 64 ...\n",
        "NUM_WORKERS = 2 # Number of cpu cores\n",
        "PIN_MEMORY = True\n",
        "PERSISTENT_WORKERS = False # Set true if you have persistent workers issues\n",
        "# Create dataset and dataloader with augmentations\n",
        "train_dataset = LoveDADataset(root_dir='Train/', split='Train', transform_image=transform_image, transform_label=transform_label, subdir='Urban', aug1=AUG1, aug2=AUG2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=PERSISTENT_WORKERS)\n",
        "\n",
        "\n",
        "val_dataset = LoveDADataset(root_dir='Val/', split='Val', transform_image=transform_image, transform_label=transform_label, subdir='Rural')\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# Visualize the first batch of images\n",
        "for i, (image, label) in enumerate(train_loader):\n",
        "    if i == 0:  # Show only the first batch\n",
        "        show_image(image[0])  # Access the first image of the batch\n",
        "        show_mask(label[0])\n",
        "        break\n",
        "\n",
        "\n",
        "# # Definisci una dimensione più piccola per il subset (ad esempio 10% del dataset)\n",
        "# subset_size = 100  # Usa solo i primi 100 campioni per il test\n",
        "\n",
        "# # Usa torch.utils.data.Subset per ottenere un subset casuale dei dati\n",
        "# subset_indices = torch.randperm(len(train_loader.dataset)).tolist()[:subset_size]\n",
        "# train_subset = torch.utils.data.Subset(train_loader.dataset, subset_indices)\n",
        "\n",
        "# # Crea un DataLoader per il subset\n",
        "# #DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=PERSISTENT_WORKERS)\n",
        "# train_subset_loader = DataLoader(train_subset, batch_size=2, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=PERSISTENT_WORKERS)\n",
        "# for i, (image, label) in enumerate(train_subset_loader):\n",
        "#     if i == 0:  # Show only the first batch\n",
        "#         show_image(image[0])  # Access the first image of the batch\n",
        "#         show_mask(label[0])\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3BRmkE1yTR6"
      },
      "outputs": [],
      "source": [
        "# Funzione per visualizzare immagini con augmentations applicate\n",
        "def visualize_augmented_images(dataloader, num_images=1):\n",
        "    \"\"\"\n",
        "    Visualizza immagini con augmentations applicate.\n",
        "    Args:\n",
        "        dataloader: Il DataLoader dal quale prelevare le immagini.\n",
        "        num_images: Numero di immagini da visualizzare.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for images, labels in dataloader:\n",
        "        for i in range(images.size(0)):\n",
        "            if count >= num_images:\n",
        "                return  # Mostra solo il numero richiesto di immagini\n",
        "            print(f\"Visualizing image {count + 1}\")\n",
        "            show_image(images[i])  # Visualizza l'immagine\n",
        "            show_mask(labels[i])  # Visualizza la maschera\n",
        "            count += 1\n",
        "\n",
        "# Visualizza immagini con augmentations dal train_loader\n",
        "visualize_augmented_images(train_loader, num_images=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlgiDgb2yTR6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/XuJiacong/PIDNet.git\n",
        "%cd content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20kIqimyTR6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/PIDNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcMD4-xZyTR7"
      },
      "outputs": [],
      "source": [
        "from models.model_utils import BasicBlock, Bottleneck, segmenthead, DAPPM, PAPPM, PagFM, Bag, Light_Bag\n",
        "from utils.criterion import CrossEntropy, OhemCrossEntropy, BondaryLoss\n",
        "\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_\n",
        "\n",
        "\n",
        "def get_seg_model(name, num_classes, imgnet_pretrained, model_pth):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "        print(\"vamos\")\n",
        "        pretrained_state = torch.load(model_pth, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    else:\n",
        "        # print(\"vamos\")\n",
        "\n",
        "        pretrained_dict = torch.load(model_pth, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_seg_model(name='pidnet_s', num_classes=7, imgnet_pretrained=True, model_pth='PIDNet_S_ImageNet.pth.tar').to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl6Spc8CyTR7"
      },
      "outputs": [],
      "source": [
        "num_classes=7\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "\n",
        "# random - experience\n",
        "class_weights = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345,\n",
        "                                        1.0166, 0.9969, 0.9754]).cuda()\n",
        "\n",
        "# Define the loss function\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = OhemCrossEntropy(ignore_label=255,\n",
        "                                        thres=0.9,\n",
        "                                        min_kept=131072,\n",
        "                                        weight=class_weights)\n",
        "\n",
        "# Define mIoU metric\n",
        "jaccard = JaccardIndex(task=\"multiclass\", num_classes=7).to('cuda')  # intersection over union. Directly measures the overlap between predicted segmentation and ground truth.\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20 #20\n",
        "best_miou = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") # train_loader\n",
        "    for images, masks in train_progress_bar:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        main_output = outputs[1]\n",
        "\n",
        "        # Upsample model outputs to match the target resolution (512x512)\n",
        "        outputs_resized = F.interpolate(main_output, size=(masks.shape[2], masks.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Ensure masks are in the right shape and type (long)\n",
        "        masks = masks.squeeze(1)  # Remove extra channel dimension and convert to long\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs_resized, masks)  # Use CrossEntropyLoss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update tqdm progress bar with real-time loss using set_postfix\n",
        "        train_progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader) #len(train_subset_loader) #len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_miou = 0.0\n",
        "    val_class_iou = torch.zeros(len(LABEL_MAP)).to('cuda')  # Store IoU per class\n",
        "    val_class_counts = torch.zeros(len(LABEL_MAP)).to('cuda')  # To track number of pixels for each class\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to('cuda'), masks.to('cuda')\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # [batch_size, num_classes, 64, 64]\n",
        "            main_output = outputs[1]\n",
        "            preds = torch.argmax(main_output, dim=1)  # [batch_size, 64, 64]\n",
        "\n",
        "            # Convert masks to class indices format if one-hot encoded\n",
        "            if masks.ndim == 4:  # [batch_size, num_classes, height, width]\n",
        "                masks = masks.argmax(dim=1)  # [batch_size, height, width]\n",
        "\n",
        "            # Resize preds to match the size of the masks\n",
        "            preds_resized = F.interpolate(preds.unsqueeze(1).float(), size=masks.shape[1:], mode='nearest').squeeze(1).long()\n",
        "\n",
        "            # Calculate per-class IoU\n",
        "            for c, class_name in LABEL_MAP.items():  # Iterate over LABEL_MAP classes\n",
        "                true_class = (masks == class_name)\n",
        "                pred_class = (preds_resized == class_name)\n",
        "\n",
        "                intersection = torch.sum(true_class & pred_class).float()\n",
        "                union = torch.sum(true_class | pred_class).float()\n",
        "\n",
        "                if union != 0:\n",
        "                    val_class_iou[class_name] += intersection / union\n",
        "                val_class_counts[class_name] += 1\n",
        "\n",
        "            # Calculate overall mIoU for this batch\n",
        "            val_miou += jaccard(preds_resized, masks)\n",
        "\n",
        "    # Average metrics\n",
        "    val_miou /= len(val_loader)\n",
        "\n",
        "    # Calculate average IoU for each class\n",
        "    avg_class_iou = val_class_iou / val_class_counts\n",
        "\n",
        "    # Print validation metrics at the end of each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation mIoU: {val_miou:.4f}\")\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Per-Class IoU: {dict(zip(LABEL_MAP.keys(), avg_class_iou.tolist()))}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_miou > best_miou:\n",
        "        best_miou = val_miou\n",
        "        torch.save(model.state_dict(), 'best_pidnet_model.pth')\n",
        "\n",
        "print(f\"Best Validation mIoU: {best_miou:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfa3csockYui"
      },
      "outputs": [],
      "source": [
        "TRAINING=1\n",
        "if TRAINING:  # Ensure you have a condition for training mode\n",
        "    # Pick a random batch from the train loader\n",
        "    a, b = val_dataset[330]\n",
        "    inputs, targets = a, b  # Assuming the loader returns (input, target) pairs\n",
        "\n",
        "    # Move the inputs and targets to the same device as the model\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    inputs = inputs.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "    # Get the model output for the input\n",
        "    model.eval()  # Make sure the model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        output = model(inputs)  # Get model prediction for the input\n",
        "\n",
        "    output = output[1]\n",
        "\n",
        "    # Print the random input, output, and ground truth\n",
        "    print(\"\\nRandom Sample (Epoch {})\".format(i))\n",
        "    print(\"Input Tensor: \", inputs.shape)\n",
        "    print(\"Ground Truth: \", targets.shape)\n",
        "    print(\"Model Output: \", len(output))\n",
        "\n",
        "    show_image(a)\n",
        "\n",
        "    plt.imshow(b.argmax(dim=0)) #grouth truth mask\n",
        "\n",
        "    output = output.squeeze(0)\n",
        "\n",
        "    plt.imshow(output.cpu().argmax(dim=0).numpy()) #\n",
        "\n",
        "    fig, axes = plt.subplots(1, 7, figsize=(20, 5))\n",
        "\n",
        "    # Itera attraverso i canali della maschera (7 classi)\n",
        "    for i in range(7):\n",
        "        # Estrai il canale i-esimo\n",
        "        mask_class = output[i].cpu().numpy()  # Converti in array numpy per la visualizzazione\n",
        "\n",
        "        # Visualizza la maschera della classe i\n",
        "        axes[i].imshow(mask_class, cmap='jet')  # Usa una mappa di colori (ad esempio, 'jet')\n",
        "        axes[i].set_title(f'Class {i + 1}')\n",
        "        axes[i].axis('off')  # Disabilita gli assi\n",
        "\n",
        "    # Mostra il risultato\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 7, figsize=(20, 5))\n",
        "\n",
        "    # Itera attraverso i canali della maschera (7 classi)\n",
        "    for i in range(7):\n",
        "        # Estrai il canale i-esimo\n",
        "        mask_class = targets[i].cpu().numpy()  # Converti in array numpy per la visualizzazione\n",
        "\n",
        "        # Visualizza la maschera della classe i\n",
        "        axes[i].imshow(mask_class, cmap='jet')  # Usa una mappa di colori (ad esempio, 'jet')\n",
        "        axes[i].set_title(f'Class {i + 1}')\n",
        "        axes[i].axis('off')  # Disabilita gli assi\n",
        "\n",
        "    # Mostra il risultato\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GZHZqqbj4WN"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Calcolo della Loss ---\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Definisci la funzione di perdita\n",
        "# La loss richiede che il target sia di tipo (B, H, W), quindi rimuovi il one-hot encoding\n",
        "targets_class = targets.argmax(dim=0).unsqueeze(0)  # Ground truth in formato (B, H, W)\n",
        "\n",
        "# --- Upsample l'output del modello ---\n",
        "# Porta l'output del modello alla stessa risoluzione del target (512 x 512)\n",
        "output_resized = F.interpolate(output.unsqueeze(0), size=targets_class.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "loss = criterion(output_resized, targets_class)  # Calcola la loss\n",
        "\n",
        "print(f\"Loss per l'immagine corrente: {loss.item()}\")\n",
        "\n",
        "# --- Calcolo della mIoU ---\n",
        "def compute_miou(pred_mask, true_mask, num_classes):\n",
        "    iou_list = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred_mask == cls)  # Predizione per la classe cls\n",
        "        true_cls = (true_mask == cls)  # Ground truth per la classe cls\n",
        "\n",
        "        intersection = (pred_cls & true_cls).sum().item()\n",
        "        union = (pred_cls | true_cls).sum().item()\n",
        "\n",
        "        if union == 0:\n",
        "            iou = float('nan')  # Evita la divisione per zero\n",
        "        else:\n",
        "            iou = intersection / union\n",
        "\n",
        "        iou_list.append(iou)\n",
        "\n",
        "    miou = torch.tensor(iou_list).nanmean().item()  # Calcola la media ignorando NaN\n",
        "    return miou\n",
        "\n",
        "# Calcola la mIoU\n",
        "predicted_mask_resized = output_resized.squeeze(0).argmax(dim=0).cpu()  # Maschera predetta upscalata\n",
        "true_mask = targets_class.squeeze(0).cpu()  # Ground truth\n",
        "num_classes = 7  # Numero di classi\n",
        "\n",
        "miou = compute_miou(predicted_mask_resized, true_mask, num_classes)\n",
        "print(f\"Mean IoU per l'immagine corrente: {miou}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5syfrNlWj4WN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}